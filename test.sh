#!/bin/bash
# ==============================================================================
# RDT-1B æœ€å°è®­ç»ƒæµ‹è¯•è„šæœ¬
# ==============================================================================
# ç›®çš„ï¼šå¿«é€ŸéªŒè¯è®­ç»ƒpipelineæ˜¯å¦æ­£å¸¸å·¥ä½œ
# é¢„è®¡è¿è¡Œæ—¶é—´ï¼š5-10åˆ†é’Ÿ
# ==============================================================================

export run_id="test_$(date +%Y%m%d_%H%M%S)"

# ====== NCCL é…ç½®ï¼ˆå•å¡æˆ–åŒå¡æµ‹è¯•ï¼‰======
export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_7:1,mlx5_8:1,mlx5_9:1
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=bond0
export NCCL_DEBUG=WARN  # å‡å°‘æ—¥å¿—è¾“å‡º
export NCCL_NVLS_ENABLE=0
export DS_BUILD_EVOFORMER_ATTN=0

# ====== çŽ¯å¢ƒé…ç½® ======
export CFLAGS="-I/usr/include"
export LDFLAGS="-L/usr/lib/x86_64-linux-gnu"
export TEXT_ENCODER_NAME="google/t5-v1_1-xxl"
export VISION_ENCODER_NAME="google/siglip-so400m-patch14-384"

# ====== æµ‹è¯•é…ç½®ï¼ˆæœ€å°åŒ–ï¼‰======
dataset_name="baai"
action_name="action176"
test_name="minimal_test"

# æœ€å°åŒ–è¶…å‚æ•°
train_batch_size=2           # æ¯å¡åª2ä¸ªæ ·æœ¬ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
gradient_accumulation_steps=1 # ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
sample_batch_size=2          # é‡‡æ ·batch size
num_sample_batches=1         # åªé‡‡æ ·1ä¸ªbatch
seed=42
max_train_steps=20           # åªè®­ç»ƒ20æ­¥ï¼
checkpointing_period=10      # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡
sample_period=5              # æ¯5æ­¥é‡‡æ ·ä¸€æ¬¡
lr="1e-4"

export OUTPUT_DIR="./checkpoints/TEST_${test_name}_${run_id}"

# ====== åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½• ======
mkdir -p "$OUTPUT_DIR"
echo "âœ… æµ‹è¯•è¾“å‡ºç›®å½•: $OUTPUT_DIR"

cat > "$OUTPUT_DIR/test_config.txt" <<EOF
RDT-1B Minimal Training Test
======================================
Run ID: ${run_id}
Purpose: Verify training pipeline
Expected Duration: 5-10 minutes
Max Steps: ${max_train_steps}

Test Configuration:
  - Batch Size: ${train_batch_size}
  - Gradient Accumulation: ${gradient_accumulation_steps}
  - Max Steps: ${max_train_steps}
  - Checkpoint Every: ${checkpointing_period} steps
  - Sample Every: ${sample_period} steps

What to Check:
  âœ“ Data loading works
  âœ“ Model forward pass works
  âœ“ Loss computation works
  âœ“ Backward pass works
  âœ“ Optimizer step works
  âœ“ Checkpoint saving works
  âœ“ Sampling works

Started: $(date)
EOF

# ====== æ˜¾ç¤ºæµ‹è¯•ä¿¡æ¯ ======
echo ""
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘              ðŸ§ª RDT-1B Minimal Training Test                     â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ðŸ“‹ æµ‹è¯•ç›®çš„: éªŒè¯è®­ç»ƒpipelineæ˜¯å¦æ­£å¸¸å·¥ä½œ"
echo "â±ï¸  é¢„è®¡æ—¶é—´: 5-10åˆ†é’Ÿ"
echo "ðŸ“Š è®­ç»ƒæ­¥æ•°: ${max_train_steps} steps"
echo "ðŸ’¾ è¾“å‡ºç›®å½•: ${OUTPUT_DIR}"
echo ""
echo "ðŸ” å°†è¦éªŒè¯çš„ç»„ä»¶:"
echo "  âœ“ æ•°æ®åŠ è½½ï¼ˆBSONæ ¼å¼ï¼‰"
echo "  âœ“ å¤šæ¨¡æ€ç¼–ç ï¼ˆT5 + SigLIPï¼‰"
echo "  âœ“ æ¨¡åž‹å‰å‘ä¼ æ’­"
echo "  âœ“ æŸå¤±è®¡ç®—"
echo "  âœ“ åå‘ä¼ æ’­"
echo "  âœ“ ä¼˜åŒ–å™¨æ›´æ–°"
echo "  âœ“ Checkpointä¿å­˜"
echo "  âœ“ é‡‡æ ·è¯„ä¼°"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ðŸš€ å¼€å§‹æµ‹è¯•..."
echo ""

# ====== å¯åŠ¨æœ€å°æµ‹è¯• ======
# ä½¿ç”¨å•å¡æˆ–åŒå¡æµ‹è¯•ï¼ˆæŽ’é™¤GPU:0ï¼‰
# CUDA_VISIBLE_DEVICES=0 python main_baai.py \
deepspeed --include="localhost:1" main_baai.py \
    --deepspeed="./configs/zero2.json" \
    --pretrained_model_name_or_path="./checkpoints/rdt-1b" \
    --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
    --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
    --output_dir=$OUTPUT_DIR \
    --seed=${seed} \
    --train_batch_size=${train_batch_size} \
    --gradient_accumulation_steps=${gradient_accumulation_steps} \
    --sample_batch_size=${sample_batch_size} \
    --num_sample_batches=${num_sample_batches} \
    --max_train_steps=${max_train_steps} \
    --checkpointing_period=${checkpointing_period} \
    --sample_period=${sample_period} \
    --checkpoints_total_limit=3 \
    --lr_scheduler="constant" \
    --learning_rate=${lr} \
    --mixed_precision="bf16" \
    --dataloader_num_workers=4 \
    --image_aug \
    --dataset_type="finetune" \
    --state_noise_snr=40 \
    --load_from_bson \
    --report_to=tensorboard \
    --precomp_lang_embed
    # --use_8bit_adam

EXIT_CODE=$?

# ====== æµ‹è¯•ç»“æžœæŠ¥å‘Š ======
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

if [ $EXIT_CODE -eq 0 ]; then
    echo "âœ… æµ‹è¯•å®Œæˆï¼"
    echo ""
    echo "ðŸ“Š æ£€æŸ¥æµ‹è¯•ç»“æžœ:"
    echo ""
    
    # æ£€æŸ¥checkpointæ˜¯å¦ç”Ÿæˆ
    if [ -d "$OUTPUT_DIR/checkpoint-10" ] || [ -d "$OUTPUT_DIR/checkpoint-20" ]; then
        echo "  âœ… Checkpointä¿å­˜æˆåŠŸ"
        ls -lh "$OUTPUT_DIR" | grep checkpoint
    else
        echo "  âš ï¸  æœªæ‰¾åˆ°checkpointæ–‡ä»¶"
    fi
    
    echo ""
    
    # æ£€æŸ¥TensorBoardæ—¥å¿—
    if [ -d "$OUTPUT_DIR/logs" ]; then
        echo "  âœ… TensorBoardæ—¥å¿—å·²ç”Ÿæˆ"
        echo "     æŸ¥çœ‹æ–¹æ³•: tensorboard --logdir=$OUTPUT_DIR"
    else
        echo "  âš ï¸  æœªæ‰¾åˆ°TensorBoardæ—¥å¿—"
    fi
    
    echo ""
    echo "ðŸ“ æµ‹è¯•ç»“æžœä½ç½®: $OUTPUT_DIR"
    echo ""
    echo "ðŸŽ‰ æ‰€æœ‰ç»„ä»¶å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒï¼"
    
else
    echo "âŒ æµ‹è¯•å¤±è´¥ (Exit Code: $EXIT_CODE)"
    echo ""
    echo "è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜ã€‚"
    echo ""
    echo "å¸¸è§é—®é¢˜:"
    echo "  1. é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Ÿ"
    echo "  2. BSONæ•°æ®æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ï¼Ÿ"
    echo "  3. GPUæ˜¾å­˜æ˜¯å¦å……è¶³ï¼Ÿ"
    echo "  4. ä¾èµ–åŒ…æ˜¯å¦å®Œæ•´å®‰è£…ï¼Ÿ"
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# å®Œæˆæç¤ºéŸ³ï¼ˆå¦‚æžœæ”¯æŒï¼‰
echo -e "\a"

exit $EXIT_CODE