#!/bin/bash
# ====================================
# RDT-1B LoRA å¿«é€Ÿæµ‹è¯•è„šæœ¬
# ç”¨é€”ï¼šéªŒè¯è®­ç»ƒæµç¨‹æ˜¯å¦æ­£å¸¸ï¼Œä¸è¿›è¡Œå®Œæ•´è®­ç»ƒ
# ====================================

export run_id=$(date +%Y%m%d_%H%M%S)
export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_7:1,mlx5_8:1,mlx5_9:1
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=bond0
export NCCL_DEBUG=INFO
export NCCL_NVLS_ENABLE=0
export DS_BUILD_EVOFORMER_ATTN=0

# è§£å†³åˆ†å¸ƒå¼è®­ç»ƒç½‘ç»œé—®é¢˜
export MASTER_ADDR=127.0.0.1      # å¼ºåˆ¶ä½¿ç”¨IPv4
export MASTER_PORT=29601           # ä½¿ç”¨ç‹¬ç«‹ç«¯å£é¿å…ä¸Žå…¶ä»–deepspeedè¿›ç¨‹å†²çª
export NCCL_SOCKET_FAMILY=AF_INET  # ç¦ç”¨IPv6ï¼Œåªä½¿ç”¨IPv4

export TEXT_ENCODER_NAME="google/t5-v1_1-xxl"
export VISION_ENCODER_NAME="google/siglip-so400m-patch14-384"
export CFLAGS="-I/usr/include"
export LDFLAGS="-L/usr/lib/x86_64-linux-gnu"

# ====== æµ‹è¯•é…ç½®ï¼ˆèµ„æºæ¶ˆè€—å°ï¼Œå¿«é€ŸéªŒè¯ï¼‰ ======
dataset_name="baai"
model_type="lora"
lora_r=32
lora_alpha=64
lr="1e-4"
batch_size=4          # æµ‹è¯•ç”¨å°batch size
seed=42
test_steps=20         # åªè·‘20æ­¥æµ‹è¯•

# æµ‹è¯•è¾“å‡ºè·¯å¾„
export TEST_OUTPUT_DIR="./checkpoints/test-rdt1b-lora-${run_id}"
echo "============================================"
echo "ðŸ§ª RDT-1B LoRA å¿«é€Ÿæµ‹è¯•æ¨¡å¼"
echo "============================================"
echo "æµ‹è¯•æ­¥æ•°: ${test_steps} steps"
echo "Batch Size: ${batch_size}"
echo "è¾“å‡ºç›®å½•: ${TEST_OUTPUT_DIR}"
echo "============================================"
# ============================================

if [ ! -d "$TEST_OUTPUT_DIR" ]; then
    mkdir -p "$TEST_OUTPUT_DIR"
    echo "âœ… æµ‹è¯•è¾“å‡ºæ–‡ä»¶å¤¹å·²åˆ›å»º: '$TEST_OUTPUT_DIR'"
    
    # åˆ›å»ºæµ‹è¯•é…ç½®è¯´æ˜Ž
    cat > "$TEST_OUTPUT_DIR/test_config.txt" <<EOF
RDT-1B LoRA å¿«é€Ÿæµ‹è¯•é…ç½®
======================================
æµ‹è¯•ç›®çš„: éªŒè¯è®­ç»ƒæµç¨‹æ˜¯å¦æ­£å¸¸
Run ID: ${run_id}
Model: RDT-1B
Method: LoRA Fine-tuning
Dataset: ${dataset_name}

æµ‹è¯•å‚æ•°:
  - Test Steps: ${test_steps} (æ­£å¼è®­ç»ƒ: 200000)
  - Batch Size: ${batch_size} (æ­£å¼è®­ç»ƒ: 32)
  - GPUs: 2 (æ­£å¼è®­ç»ƒ: 7-8)
  - Random Seed: ${seed}

LoRA Parameters:
  - Rank: ${lora_r}
  - Alpha: ${lora_alpha}
  - Dropout: 0.1
  - Target Modules: all

Training Hyperparameters:
  - Learning Rate: ${lr}
  - Mixed Precision: bf16
  
Command: bash test.sh
Started: $(date)

æ³¨æ„: è¿™æ˜¯æµ‹è¯•è¿è¡Œï¼Œä¸ä¼šäº§ç”Ÿå¯ç”¨çš„è®­ç»ƒæ¨¡åž‹ï¼
EOF
else
    echo "âš ï¸  æµ‹è¯•è¾“å‡ºæ–‡ä»¶å¤¹å·²å­˜åœ¨: '$TEST_OUTPUT_DIR'"
fi

# ====== å¿«é€Ÿæµ‹è¯•ï¼šåªä½¿ç”¨GPU 0 å•å¡è¿è¡Œï¼ˆä¸ä½¿ç”¨DeepSpeedï¼‰ ======
echo ""
echo "ðŸš€ å¼€å§‹æµ‹è¯• (GPU 0 å•å¡æ¨¡å¼ï¼Œä¸ä½¿ç”¨DeepSpeed)..."
echo ""

CUDA_VISIBLE_DEVICES=0 python main_baai_lora.py \
    --pretrained_model_name_or_path="./checkpoints/rdt-1b" \
    --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
    --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
    --output_dir=$TEST_OUTPUT_DIR \
    --seed=${seed} \
    --use_lora \
    --lora_rank=${lora_r} \
    --lora_alpha=${lora_alpha} \
    --lora_dropout=0.1 \
    --lora_target_modules="all" \
    --train_batch_size=${batch_size} \
    --sample_batch_size=${batch_size} \
    --num_sample_batches=1 \
    --max_train_steps=${test_steps} \
    --checkpointing_period=10 \
    --sample_period=10 \
    --checkpoints_total_limit=2 \
    --lr_scheduler="constant" \
    --learning_rate=${lr} \
    --mixed_precision="bf16" \
    --dataloader_num_workers=4 \
    --dataset_type="finetune" \
    --state_noise_snr=40 \
    --load_from_bson \
    --report_to=tensorboard \
    --precomp_lang_embed

# æ£€æŸ¥é€€å‡ºçŠ¶æ€
if [ $? -eq 0 ]; then
    echo ""
    echo "============================================"
    echo "âœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼"
    echo "============================================"
    echo "è®­ç»ƒæµç¨‹éªŒè¯é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒã€‚"
    echo "ä½¿ç”¨å‘½ä»¤: bash train_baai_lora.sh"
    echo ""
    echo "æµ‹è¯•è¾“å‡ºä½ç½®: ${TEST_OUTPUT_DIR}"
    echo "============================================"
else
    echo ""
    echo "============================================"
    echo "âŒ æµ‹è¯•å¤±è´¥ï¼"
    echo "============================================"
    echo "è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é…ç½®ã€‚"
    echo "å¸¸è§é—®é¢˜ï¼š"
    echo "  1. æ£€æŸ¥é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„æ˜¯å¦æ­£ç¡®"
    echo "  2. æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®"
    echo "  3. æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"
    echo "  4. æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…å®Œæ•´"
    echo "============================================"
    exit 1
fi

