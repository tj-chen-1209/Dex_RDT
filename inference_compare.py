#!/usr/bin/env python3
"""
RDT-1B æ¨ç†å¯¹æ¯”è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»lerobot_baaiæ•°æ®é›†éšæœºé€‰å–ä¸€ä¸ªepisodeçš„æŸä¸ªframe
2. ä½¿ç”¨è®­ç»ƒå¥½çš„checkpointè¿›è¡Œæ¨ç†
3. å¯¹æ¯”é¢„æµ‹çš„action chunkä¸çœŸå®çš„action chunk
4. ç»˜åˆ¶å…³èŠ‚è§’å¯¹æ¯”å›¾

Author: AI Assistant
"""

import os
import sys
import json
import yaml
import random
import argparse
from pathlib import Path

import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from configs.state_vec import STATE_VEC_IDX_MAPPING
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.rdt_runner import RDTRunner


# ============================================================================
# é…ç½®å’Œå¸¸é‡
# ============================================================================

# çŠ¶æ€å‘é‡ç´¢å¼•æ˜ å°„ï¼š36ç»´åŠ¨ä½œå‘é‡ -> 128ç»´ç»Ÿä¸€å‘é‡
# é¡ºåºä¸info.jsonä¸­çš„actionä¸€è‡´ï¼šå³è‡‚6 + å³æ‰‹12 + å·¦è‡‚6 + å·¦æ‰‹12
BAAI_STATE_INDICES = [
    STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(6)
] + [
    STATE_VEC_IDX_MAPPING[f"right_hand_joint_{i}_pos"] for i in range(12)
] + [
    STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"] for i in range(6)
] + [
    STATE_VEC_IDX_MAPPING[f"left_hand_joint_{i}_pos"] for i in range(12)
]

# å…³èŠ‚åç§°ï¼ˆç”¨äºç»˜å›¾ï¼‰
JOINT_NAMES = [
    # å³è‡‚ (0-5)
    "R_Arm_J0", "R_Arm_J1", "R_Arm_J2", "R_Arm_J3", "R_Arm_J4", "R_Arm_J5",
    # å³æ‰‹ (6-17)
    "R_Hand_J0", "R_Hand_J1", "R_Hand_J2", "R_Hand_J3", "R_Hand_J4", "R_Hand_J5",
    "R_Hand_J6", "R_Hand_J7", "R_Hand_J8", "R_Hand_J9", "R_Hand_J10", "R_Hand_J11",
    # å·¦è‡‚ (18-23)
    "L_Arm_J0", "L_Arm_J1", "L_Arm_J2", "L_Arm_J3", "L_Arm_J4", "L_Arm_J5",
    # å·¦æ‰‹ (24-35)
    "L_Hand_J0", "L_Hand_J1", "L_Hand_J2", "L_Hand_J3", "L_Hand_J4", "L_Hand_J5",
    "L_Hand_J6", "L_Hand_J7", "L_Hand_J8", "L_Hand_J9", "L_Hand_J10", "L_Hand_J11",
]


# ============================================================================
# æ¨¡å‹åŠ è½½
# ============================================================================

class BAAIInferenceModel:
    """ç”¨äºæ¨ç†çš„RDTæ¨¡å‹å°è£…ç±»"""
    
    def __init__(
        self,
        checkpoint_path: str,
        config_path: str = "configs/base.yaml",
        vision_encoder_path: str = "google/siglip-so400m-patch14-384",
        device: str = "cuda",
        dtype: torch.dtype = torch.bfloat16,
        control_frequency: int = 20,
    ):
        self.device = device
        self.dtype = dtype
        self.control_frequency = control_frequency
        
        print("=" * 70)
        print("ğŸš€ åˆå§‹åŒ–æ¨ç†æ¨¡å‹")
        print("=" * 70)
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        print(f"ğŸ“‚ Checkpoint: {checkpoint_path}")
        print(f"ğŸ“‚ Config: {config_path}")
        print(f"ğŸ“‚ Vision Encoder: {vision_encoder_path}")
        
        # åŠ è½½è§†è§‰ç¼–ç å™¨
        print("\nğŸ”„ åŠ è½½è§†è§‰ç¼–ç å™¨...")
        self.vision_encoder = SiglipVisionTower(
            vision_tower=vision_encoder_path, 
            args=None
        )
        self.image_processor = self.vision_encoder.image_processor
        self.vision_encoder = self.vision_encoder.to(device, dtype=dtype)
        self.vision_encoder.eval()
        print(f"   âœ… SigLIPå·²åŠ è½½, num_patches={self.vision_encoder.num_patches}")
        
        # è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦
        img_cond_len = (
            self.config["common"]["img_history_size"] 
            * self.config["common"]["num_cameras"] 
            * self.vision_encoder.num_patches
        )
        
        # åˆ›å»ºRDTæ¨¡å‹
        print("\nğŸ”„ åˆ›å»ºRDTæ¨¡å‹...")
        self.policy = RDTRunner(
            action_dim=self.config["common"]["state_dim"],
            pred_horizon=self.config["common"]["action_chunk_size"],
            config=self.config["model"],
            lang_token_dim=self.config["model"]["lang_token_dim"],
            img_token_dim=self.config["model"]["img_token_dim"],
            state_token_dim=self.config["model"]["state_token_dim"],
            max_lang_cond_len=self.config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (
                    self.config["common"]["img_history_size"],
                    self.config["common"]["num_cameras"],
                    -self.vision_encoder.num_patches
                )),
            ],
            lang_pos_embed_config=[
                ("lang", -self.config["dataset"]["tokenizer_max_length"]),
            ],
            dtype=dtype,
        )
        
        # åŠ è½½checkpointæƒé‡
        print("\nğŸ”„ åŠ è½½checkpointæƒé‡...")
        self._load_checkpoint(checkpoint_path)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.policy = self.policy.to(device, dtype=dtype)
        self.policy.eval()
        
        print("\nâœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")
        print("=" * 70)
    
    def _load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½checkpointæƒé‡"""
        checkpoint_file = Path(checkpoint_path) / "pytorch_model.bin"
        
        if not checkpoint_file.exists():
            # å°è¯•åŠ è½½é›¶ç¢çš„DeepSpeed checkpoint
            print(f"   âš ï¸  æœªæ‰¾åˆ° pytorch_model.binï¼Œå°è¯•ä»DeepSpeedæ ¼å¼åŠ è½½...")
            zero_to_fp32_path = Path(checkpoint_path) / "zero_to_fp32.py"
            if zero_to_fp32_path.exists():
                raise NotImplementedError(
                    f"è¯·å…ˆè¿è¡Œ python {zero_to_fp32_path} {checkpoint_path} {checkpoint_path}/pytorch_model.bin æ¥è½¬æ¢æƒé‡"
                )
            raise FileNotFoundError(f"æœªæ‰¾åˆ°checkpointæ–‡ä»¶: {checkpoint_file}")
        
        print(f"   ğŸ“¦ åŠ è½½æƒé‡: {checkpoint_file}")
        state_dict = torch.load(checkpoint_file, map_location='cpu')
        
        # å¤„ç†DeepSpeedä¿å­˜çš„state_dictæ ¼å¼
        if "module" in state_dict:
            state_dict = state_dict["module"]
        
        # ç§»é™¤å¯èƒ½çš„ "module." å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
        
        self.policy.load_state_dict(new_state_dict, strict=False)
        print(f"   âœ… æƒé‡åŠ è½½æˆåŠŸ!")
    
    def _format_state_to_unified(self, state_36: np.ndarray) -> np.ndarray:
        """å°†36ç»´çŠ¶æ€å‘é‡æ˜ å°„åˆ°128ç»´ç»Ÿä¸€å‘é‡"""
        if state_36.ndim == 1:
            state_36 = state_36[np.newaxis, :]
        
        B, D = state_36.shape
        state_128 = np.zeros((B, self.config["common"]["state_dim"]))
        state_128[:, BAAI_STATE_INDICES] = state_36
        return state_128
    
    def _unformat_unified_to_state(self, action_128: np.ndarray) -> np.ndarray:
        """å°†128ç»´ç»Ÿä¸€å‘é‡æ˜ å°„å›36ç»´åŠ¨ä½œå‘é‡"""
        if action_128.ndim == 2:
            return action_128[:, BAAI_STATE_INDICES]
        elif action_128.ndim == 3:
            return action_128[:, :, BAAI_STATE_INDICES]
        return action_128[BAAI_STATE_INDICES]
    
    def preprocess_images(self, images: list) -> torch.Tensor:
        """
        é¢„å¤„ç†å›¾åƒåˆ—è¡¨
        
        Args:
            images: å›¾åƒåˆ—è¡¨ï¼Œé¡ºåºä¸º [head_t-1, right_wrist_t-1, left_wrist_t-1, 
                                      head_t, right_wrist_t, left_wrist_t]
                    å…±6å¼ å›¾åƒ (2ä¸ªæ—¶é—´æ­¥ x 3ä¸ªç›¸æœº)
        
        Returns:
            torch.Tensor: ç¼–ç åçš„å›¾åƒç‰¹å¾ (1, num_patches*6, hidden_size)
        """
        background_color = np.array([
            int(x * 255) for x in self.image_processor.image_mean
        ], dtype=np.uint8).reshape(1, 1, 3)
        background_image = np.ones((
            self.image_processor.size["height"],
            self.image_processor.size["width"], 3
        ), dtype=np.uint8) * background_color
        
        image_tensors = []
        for img in images:
            if img is None:
                img = Image.fromarray(background_image)
            elif isinstance(img, np.ndarray):
                img = Image.fromarray(img)
            
            # Pad to square
            width, height = img.size
            if width != height:
                size = max(width, height)
                new_img = Image.new(
                    img.mode, (size, size),
                    tuple(int(x * 255) for x in self.image_processor.image_mean)
                )
                new_img.paste(img, ((size - width) // 2, (size - height) // 2))
                img = new_img
            
            # ä½¿ç”¨image_processorå¤„ç†
            processed = self.image_processor.preprocess(img, return_tensors='pt')
            image_tensors.append(processed['pixel_values'][0])
        
        # Stack and encode
        image_tensor = torch.stack(image_tensors, dim=0).to(self.device, dtype=self.dtype)
        
        with torch.no_grad():
            image_embeds = self.vision_encoder(image_tensor)
            image_embeds = image_embeds.reshape(-1, self.vision_encoder.hidden_size)
            image_embeds = image_embeds.unsqueeze(0)  # (1, N, hidden_size)
        
        return image_embeds
    
    @torch.no_grad()
    def predict(
        self,
        state_36: np.ndarray,
        images: list,
        lang_embeds: torch.Tensor,
    ) -> np.ndarray:
        """
        æ‰§è¡Œæ¨ç†ï¼Œé¢„æµ‹action chunk
        
        Args:
            state_36: å½“å‰çŠ¶æ€ (36,)
            images: å›¾åƒåˆ—è¡¨ [6å¼ å›¾åƒ]
            lang_embeds: è¯­è¨€åµŒå…¥ (seq_len, embed_dim)
        
        Returns:
            predicted_actions: (chunk_size, 36) é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
        """
        # å‡†å¤‡çŠ¶æ€
        state_128 = self._format_state_to_unified(state_36)
        state_tensor = torch.from_numpy(state_128).to(self.device, dtype=self.dtype)
        state_tensor = state_tensor.unsqueeze(1)  # (1, 1, 128)
        
        # å‡†å¤‡çŠ¶æ€mask
        state_mask = np.zeros(self.config["common"]["state_dim"])
        state_mask[BAAI_STATE_INDICES] = 1
        state_mask_tensor = torch.from_numpy(state_mask).to(self.device, dtype=self.dtype)
        state_mask_tensor = state_mask_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, 128)
        
        # ç¼–ç å›¾åƒ
        image_embeds = self.preprocess_images(images)
        
        # å‡†å¤‡è¯­è¨€æ¡ä»¶
        if lang_embeds.ndim == 2:
            lang_embeds = lang_embeds.unsqueeze(0)  # (1, seq_len, embed_dim)
        lang_embeds = lang_embeds.to(self.device, dtype=self.dtype)
        lang_attn_mask = torch.ones(
            lang_embeds.shape[:2], dtype=torch.bool, device=self.device
        )
        
        # å‡†å¤‡æ§åˆ¶é¢‘ç‡
        ctrl_freqs = torch.tensor([self.control_frequency], device=self.device)
        
        # æ‰§è¡Œæ¨ç†
        predicted_actions = self.policy.predict_action(
            lang_tokens=lang_embeds,
            lang_attn_mask=lang_attn_mask,
            img_tokens=image_embeds,
            state_tokens=state_tensor,
            action_mask=state_mask_tensor,
            ctrl_freqs=ctrl_freqs,
        )
        
        # è½¬æ¢å›numpyå¹¶æå–36ç»´
        predicted_actions = predicted_actions.squeeze(0).cpu().numpy()  # (chunk_size, 128)
        predicted_actions_36 = self._unformat_unified_to_state(predicted_actions)  # (chunk_size, 36)
        
        return predicted_actions_36


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================

def load_episode_cache(cache_dir: str, episode_idx: int) -> dict:
    """åŠ è½½ç¼“å­˜çš„episodeæ•°æ®"""
    cache_file = Path(cache_dir) / f"episode_{episode_idx:06d}.pt"
    
    if not cache_file.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°episodeç¼“å­˜: {cache_file}")
    
    # å…¼å®¹ä¸åŒnumpyç‰ˆæœ¬
    import sys
    sys.modules['numpy._core'] = np.core
    sys.modules['numpy._core.multiarray'] = np.core.multiarray
    sys.modules['numpy._core.numeric'] = getattr(np.core, 'numeric', np.core)
    
    return torch.load(cache_file, map_location='cpu', weights_only=False)


def get_sample_from_dataset(
    dataset_path: str,
    episode_idx: int = None,
    step_idx: int = None,
    chunk_size: int = 64,
    img_history_size: int = 2,
) -> dict:
    """
    ä»æ•°æ®é›†è·å–ä¸€ä¸ªæ ·æœ¬ç”¨äºæ¨ç†
    
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        episode_idx: Episodeç´¢å¼•ï¼ŒNoneè¡¨ç¤ºéšæœºé€‰æ‹©
        step_idx: æ­¥æ•°ç´¢å¼•ï¼ŒNoneè¡¨ç¤ºéšæœºé€‰æ‹©
        chunk_size: Action chunkå¤§å°
        img_history_size: å›¾åƒå†å²å¤§å°
    
    Returns:
        dict: åŒ…å« state, action_gt, images, lang_embeds, meta çš„å­—å…¸
    """
    cache_dir = Path(dataset_path) / "cache"
    
    # åŠ è½½å…ƒæ•°æ®
    meta_file = cache_dir / "episode_metadata.pt"
    import sys
    sys.modules['numpy._core'] = np.core
    sys.modules['numpy._core.multiarray'] = np.core.multiarray
    sys.modules['numpy._core.numeric'] = getattr(np.core, 'numeric', np.core)
    
    cache_data = torch.load(meta_file, map_location='cpu', weights_only=False)
    episode_data = cache_data['episode_data']
    episode_lens = cache_data['episode_lens']
    
    # é€‰æ‹©episode
    if episode_idx is None:
        weights = np.array(episode_lens) / np.sum(episode_lens)
        episode_idx = np.random.choice(len(episode_data), p=weights)
    
    episode_info = episode_data[episode_idx]
    actual_episode_idx = episode_info['episode_idx']
    
    print(f"\nğŸ“‚ é€‰æ‹© Episode {actual_episode_idx} (å†…éƒ¨ç´¢å¼•: {episode_idx})")
    
    # åŠ è½½episodeç¼“å­˜
    episode_cache = load_episode_cache(str(cache_dir), actual_episode_idx)
    
    qpos = episode_cache["state"]  # (T, 36)
    actions = episode_cache["action"]  # (T, 36)
    num_steps = episode_cache["frame_num"]
    images_info = episode_cache.get("images_info", {})
    
    print(f"   æ€»æ­¥æ•°: {num_steps}")
    
    # æ‰¾åˆ°è¿åŠ¨èµ·å§‹ç‚¹
    EPS = 1e-2
    qpos_delta = np.abs(qpos - qpos[0:1])
    indices = np.where(np.any(qpos_delta > EPS, axis=1))[0]
    first_idx = indices[0] if len(indices) > 0 else 1
    
    print(f"   è¿åŠ¨èµ·å§‹ç´¢å¼•: {first_idx}")
    
    # é€‰æ‹©æ­¥æ•°
    max_valid_step = min(num_steps - 1, num_steps - chunk_size)
    if step_idx is None:
        step_idx = random.randint(first_idx, max(first_idx, max_valid_step))
    step_idx = min(step_idx, max_valid_step)
    
    print(f"   é€‰æ‹©æ­¥æ•°ç´¢å¼•: {step_idx}")
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œ
    state = qpos[step_idx]  # (36,)
    action_gt = actions[step_idx:step_idx + chunk_size]  # (chunk_size, 36)
    
    # å¦‚æœaction_gté•¿åº¦ä¸è¶³ï¼Œç”¨æœ€åä¸€ä¸ªå¡«å……
    if len(action_gt) < chunk_size:
        pad_len = chunk_size - len(action_gt)
        action_gt = np.concatenate([action_gt, np.tile(action_gt[-1:], (pad_len, 1))], axis=0)
    
    # åŠ è½½å›¾åƒ
    def load_image(cam_key, frame_idx):
        if cam_key not in images_info:
            return np.zeros((480, 640, 3), dtype=np.uint8)
        
        cam_data = images_info[cam_key]
        if isinstance(cam_data, np.ndarray):
            if frame_idx < len(cam_data):
                img = cam_data[frame_idx]
                return img.astype(np.uint8) if img.dtype != np.uint8 else img
        return np.zeros((480, 640, 3), dtype=np.uint8)
    
    # åŠ è½½2ä¸ªæ—¶é—´æ­¥ x 3ä¸ªç›¸æœºçš„å›¾åƒ
    # é¡ºåº: [head_t-1, right_t-1, left_t-1, head_t, right_t, left_t]
    images = []
    for t_offset in [-1, 0]:
        t = max(0, step_idx + t_offset)
        images.append(load_image('camera_head', t))
        images.append(load_image('camera_right_wrist', t))
        images.append(load_image('camera_left_wrist', t))
    
    # åŠ è½½è¯­è¨€åµŒå…¥
    lang_embed_path = Path(dataset_path) / "instruction.pt"
    if lang_embed_path.exists():
        lang_embeds = torch.load(lang_embed_path, map_location='cpu')
        print(f"   è¯­è¨€åµŒå…¥: {lang_embeds.shape}")
    else:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¯­è¨€åµŒå…¥æ–‡ä»¶: {lang_embed_path}")
    
    return {
        "state": state,
        "action_gt": action_gt,
        "images": images,
        "lang_embeds": lang_embeds,
        "meta": {
            "episode_idx": actual_episode_idx,
            "step_idx": step_idx,
            "num_steps": num_steps,
        }
    }


# ============================================================================
# å¯è§†åŒ–
# ============================================================================

def plot_action_comparison(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
    save_path: str = "action_comparison.png",
    title: str = "Action Prediction vs Ground Truth",
    joint_groups: dict = None,
):
    """
    ç»˜åˆ¶é¢„æµ‹åŠ¨ä½œå’ŒçœŸå®åŠ¨ä½œçš„å¯¹æ¯”å›¾
    
    Args:
        action_gt: çœŸå®åŠ¨ä½œ (chunk_size, 36)
        action_pred: é¢„æµ‹åŠ¨ä½œ (chunk_size, 36)
        save_path: ä¿å­˜è·¯å¾„
        title: å›¾è¡¨æ ‡é¢˜
        joint_groups: å…³èŠ‚åˆ†ç»„å­—å…¸ï¼Œç”¨äºåˆ†ç»„æ˜¾ç¤º
    """
    chunk_size, num_joints = action_gt.shape
    timesteps = np.arange(chunk_size)
    
    # å®šä¹‰å…³èŠ‚åˆ†ç»„
    if joint_groups is None:
        joint_groups = {
            "å³è‡‚å…³èŠ‚ (Right Arm)": list(range(0, 6)),
            "å³æ‰‹å…³èŠ‚ (Right Hand)": list(range(6, 18)),
            "å·¦è‡‚å…³èŠ‚ (Left Arm)": list(range(18, 24)),
            "å·¦æ‰‹å…³èŠ‚ (Left Hand)": list(range(24, 36)),
        }
    
    num_groups = len(joint_groups)
    fig, axes = plt.subplots(num_groups, 1, figsize=(16, 4 * num_groups), dpi=100)
    
    if num_groups == 1:
        axes = [axes]
    
    # ä½¿ç”¨æ›´å¥½çœ‹çš„é¢œè‰²
    colors_gt = plt.cm.Blues(np.linspace(0.4, 0.9, 12))
    colors_pred = plt.cm.Oranges(np.linspace(0.4, 0.9, 12))
    
    for ax_idx, (group_name, joint_indices) in enumerate(joint_groups.items()):
        ax = axes[ax_idx]
        
        for i, joint_idx in enumerate(joint_indices):
            color_idx = i % len(colors_gt)
            
            # ç»˜åˆ¶çœŸå®å€¼
            ax.plot(
                timesteps, action_gt[:, joint_idx],
                color=colors_gt[color_idx], linestyle='-', linewidth=2,
                label=f"{JOINT_NAMES[joint_idx]} (GT)" if i < 6 else None,
                alpha=0.8
            )
            
            # ç»˜åˆ¶é¢„æµ‹å€¼
            ax.plot(
                timesteps, action_pred[:, joint_idx],
                color=colors_pred[color_idx], linestyle='--', linewidth=2,
                label=f"{JOINT_NAMES[joint_idx]} (Pred)" if i < 6 else None,
                alpha=0.8
            )
        
        ax.set_title(group_name, fontsize=14, fontweight='bold')
        ax.set_xlabel('Time Step', fontsize=12)
        ax.set_ylabel('Joint Angle (rad)', fontsize=12)
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper right', fontsize=8, ncol=2)
    
    plt.suptitle(title, fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")


def plot_detailed_comparison(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
    save_path: str = "action_comparison_detailed.png",
    meta: dict = None,
):
    """
    ç»˜åˆ¶æ¯ä¸ªå…³èŠ‚çš„è¯¦ç»†å¯¹æ¯”å›¾ï¼ˆå­å›¾å½¢å¼ï¼‰
    
    Args:
        action_gt: çœŸå®åŠ¨ä½œ (chunk_size, 36)
        action_pred: é¢„æµ‹åŠ¨ä½œ (chunk_size, 36)
        save_path: ä¿å­˜è·¯å¾„
        meta: å…ƒæ•°æ®ä¿¡æ¯
    """
    chunk_size, num_joints = action_gt.shape
    timesteps = np.arange(chunk_size)
    
    # åˆ›å»º6x6çš„å­å›¾ç½‘æ ¼
    fig, axes = plt.subplots(6, 6, figsize=(24, 20), dpi=100)
    axes = axes.flatten()
    
    # è®¡ç®—è¯¯å·®ç»Ÿè®¡
    mse = np.mean((action_gt - action_pred) ** 2, axis=0)
    mae = np.mean(np.abs(action_gt - action_pred), axis=0)
    
    for joint_idx in range(num_joints):
        ax = axes[joint_idx]
        
        # ç»˜åˆ¶çœŸå®å€¼å’Œé¢„æµ‹å€¼
        ax.plot(timesteps, action_gt[:, joint_idx], 'b-', linewidth=2, label='GT', alpha=0.8)
        ax.plot(timesteps, action_pred[:, joint_idx], 'r--', linewidth=2, label='Pred', alpha=0.8)
        
        # å¡«å……è¯¯å·®åŒºåŸŸ
        ax.fill_between(
            timesteps,
            action_gt[:, joint_idx],
            action_pred[:, joint_idx],
            alpha=0.2, color='gray'
        )
        
        ax.set_title(f"{JOINT_NAMES[joint_idx]}\nMSE: {mse[joint_idx]:.4f}", fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper right', fontsize=8)
        ax.tick_params(axis='both', labelsize=8)
    
    # æ·»åŠ æ€»ä½“æ ‡é¢˜
    title = "Action Prediction vs Ground Truth (All 36 Joints)"
    if meta:
        title += f"\nEpisode: {meta.get('episode_idx', 'N/A')}, Step: {meta.get('step_idx', 'N/A')}"
    
    plt.suptitle(title, fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š è¯¦ç»†å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    # æ‰“å°è¯¯å·®ç»Ÿè®¡
    print("\nğŸ“ˆ è¯¯å·®ç»Ÿè®¡:")
    print(f"   æ€»ä½“MSE: {np.mean(mse):.6f}")
    print(f"   æ€»ä½“MAE: {np.mean(mae):.6f}")
    print(f"   å³è‡‚MSE: {np.mean(mse[:6]):.6f}")
    print(f"   å³æ‰‹MSE: {np.mean(mse[6:18]):.6f}")
    print(f"   å·¦è‡‚MSE: {np.mean(mse[18:24]):.6f}")
    print(f"   å·¦æ‰‹MSE: {np.mean(mse[24:36]):.6f}")


def plot_error_heatmap(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
    save_path: str = "action_error_heatmap.png",
    meta: dict = None,
):
    """
    ç»˜åˆ¶è¯¯å·®çƒ­åŠ›å›¾
    
    Args:
        action_gt: çœŸå®åŠ¨ä½œ (chunk_size, 36)
        action_pred: é¢„æµ‹åŠ¨ä½œ (chunk_size, 36)
        save_path: ä¿å­˜è·¯å¾„
        meta: å…ƒæ•°æ®ä¿¡æ¯
    """
    error = np.abs(action_gt - action_pred)
    
    fig, ax = plt.subplots(figsize=(16, 10), dpi=100)
    
    im = ax.imshow(error.T, aspect='auto', cmap='RdYlBu_r', interpolation='nearest')
    
    ax.set_xlabel('Time Step', fontsize=12)
    ax.set_ylabel('Joint Index', fontsize=12)
    
    # è®¾ç½®yè½´æ ‡ç­¾
    ax.set_yticks(range(len(JOINT_NAMES)))
    ax.set_yticklabels(JOINT_NAMES, fontsize=8)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Absolute Error (rad)', fontsize=12)
    
    title = "Prediction Error Heatmap"
    if meta:
        title += f"\nEpisode: {meta.get('episode_idx', 'N/A')}, Step: {meta.get('step_idx', 'N/A')}"
    
    ax.set_title(title, fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š è¯¯å·®çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="RDT-1B æ¨ç†å¯¹æ¯”è„šæœ¬")
    parser.add_argument(
        "--checkpoint", type=str, 
        default="./checkpoints/rdt1b-full-action176-20251202_000048/checkpoint-6000",
        help="Checkpointè·¯å¾„"
    )
    parser.add_argument(
        "--dataset", type=str,
        default="./data/baai/data/lerobot_baai",
        help="æ•°æ®é›†è·¯å¾„"
    )
    parser.add_argument(
        "--config", type=str,
        default="./configs/base.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--vision_encoder", type=str,
        default="google/siglip-so400m-patch14-384",
        help="è§†è§‰ç¼–ç å™¨è·¯å¾„"
    )
    parser.add_argument(
        "--episode_idx", type=int, default=None,
        help="Episodeç´¢å¼•ï¼Œé»˜è®¤éšæœºé€‰æ‹©"
    )
    parser.add_argument(
        "--step_idx", type=int, default=None,
        help="æ­¥æ•°ç´¢å¼•ï¼Œé»˜è®¤éšæœºé€‰æ‹©"
    )
    parser.add_argument(
        "--output_dir", type=str, default="./inference_results",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device", type=str, default="cuda",
        help="è®¾å¤‡ (cuda/cpu)"
    )
    parser.add_argument(
        "--seed", type=int, default=42,
        help="éšæœºç§å­"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 70)
    print("ğŸ¯ RDT-1B æ¨ç†å¯¹æ¯”")
    print("=" * 70)
    print(f"ğŸ“‚ Checkpoint: {args.checkpoint}")
    print(f"ğŸ“‚ Dataset: {args.dataset}")
    print(f"ğŸ“‚ Output: {args.output_dir}")
    print(f"ğŸ² Seed: {args.seed}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = "cpu"
    
    if args.device == "cuda":
        print(f"ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = BAAIInferenceModel(
        checkpoint_path=args.checkpoint,
        config_path=args.config,
        vision_encoder_path=args.vision_encoder,
        device=args.device,
        dtype=torch.bfloat16 if args.device == "cuda" else torch.float32,
        control_frequency=20,  # BAAIæ•°æ®é›†çš„æ§åˆ¶é¢‘ç‡
    )
    
    # è·å–æ ·æœ¬æ•°æ®
    print("\n" + "=" * 70)
    print("ğŸ“¦ åŠ è½½æ ·æœ¬æ•°æ®")
    print("=" * 70)
    
    sample = get_sample_from_dataset(
        dataset_path=args.dataset,
        episode_idx=args.episode_idx,
        step_idx=args.step_idx,
        chunk_size=64,
        img_history_size=2,
    )
    
    state = sample["state"]
    action_gt = sample["action_gt"]
    images = sample["images"]
    lang_embeds = sample["lang_embeds"]
    meta = sample["meta"]
    
    print(f"\nğŸ“Š æ ·æœ¬ä¿¡æ¯:")
    print(f"   Episode: {meta['episode_idx']}")
    print(f"   Step: {meta['step_idx']}")
    print(f"   State shape: {state.shape}")
    print(f"   Action GT shape: {action_gt.shape}")
    print(f"   Images count: {len(images)}")
    print(f"   Lang embeds shape: {lang_embeds.shape}")
    
    # æ‰§è¡Œæ¨ç†
    print("\n" + "=" * 70)
    print("ğŸ”„ æ‰§è¡Œæ¨ç†")
    print("=" * 70)
    
    with torch.inference_mode():
        action_pred = model.predict(
            state_36=state,
            images=images,
            lang_embeds=lang_embeds,
        )
    
    print(f"âœ… æ¨ç†å®Œæˆ! é¢„æµ‹åŠ¨ä½œshape: {action_pred.shape}")
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    print("\n" + "=" * 70)
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾")
    print("=" * 70)
    
    # 1. åˆ†ç»„å¯¹æ¯”å›¾
    plot_action_comparison(
        action_gt=action_gt,
        action_pred=action_pred,
        save_path=str(output_dir / f"comparison_ep{meta['episode_idx']}_step{meta['step_idx']}.png"),
        title=f"Episode {meta['episode_idx']}, Step {meta['step_idx']}",
    )
    
    # 2. è¯¦ç»†å¯¹æ¯”å›¾ï¼ˆæ‰€æœ‰36ä¸ªå…³èŠ‚ï¼‰
    plot_detailed_comparison(
        action_gt=action_gt,
        action_pred=action_pred,
        save_path=str(output_dir / f"comparison_detailed_ep{meta['episode_idx']}_step{meta['step_idx']}.png"),
        meta=meta,
    )
    
    # 3. è¯¯å·®çƒ­åŠ›å›¾
    plot_error_heatmap(
        action_gt=action_gt,
        action_pred=action_pred,
        save_path=str(output_dir / f"error_heatmap_ep{meta['episode_idx']}_step{meta['step_idx']}.png"),
        meta=meta,
    )
    
    # ä¿å­˜æ•°å€¼ç»“æœ
    results = {
        "meta": meta,
        "action_gt": action_gt,
        "action_pred": action_pred,
        "mse_per_joint": np.mean((action_gt - action_pred) ** 2, axis=0).tolist(),
        "mae_per_joint": np.mean(np.abs(action_gt - action_pred), axis=0).tolist(),
        "total_mse": float(np.mean((action_gt - action_pred) ** 2)),
        "total_mae": float(np.mean(np.abs(action_gt - action_pred))),
    }
    
    results_path = output_dir / f"results_ep{meta['episode_idx']}_step{meta['step_idx']}.npz"
    np.savez(results_path, **results)
    print(f"\nğŸ“ æ•°å€¼ç»“æœå·²ä¿å­˜: {results_path}")
    
    print("\n" + "=" * 70)
    print("âœ… æ¨ç†å¯¹æ¯”å®Œæˆ!")
    print("=" * 70)


if __name__ == "__main__":
    main()

