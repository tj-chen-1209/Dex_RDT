#!/usr/bin/env python3
"""
RDT-1B å¼€ç¯è¯„ä¼°è„šæœ¬ (Open-Loop Evaluation)

åŠŸèƒ½ï¼š
1. åœ¨è®­ç»ƒé›†çš„å¤šä¸ªepisodeä¸Šè¿›è¡Œæ‰¹é‡æ¨ç†
2. å¯¹æ¯”é¢„æµ‹çš„action chunkä¸çœŸå®çš„action chunk
3. ç»Ÿè®¡MSEã€MAEç­‰æŒ‡æ ‡ï¼Œæ”¯æŒåˆ†å…³èŠ‚ã€åˆ†éƒ¨ä½ç»Ÿè®¡
4. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆåˆ†ç»„å¯¹æ¯”å›¾ã€çƒ­åŠ›å›¾ã€è¯¯å·®åˆ†å¸ƒå›¾ï¼‰
5. ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœä¾›åˆ†æ

Author: AI Assistant
"""

import os
import sys
import json
import yaml
import random
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional

import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(project_root))

from configs.state_vec import STATE_VEC_IDX_MAPPING
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.rdt_runner import RDTRunner


# ============================================================================
# é…ç½®å’Œå¸¸é‡
# ============================================================================

# çŠ¶æ€å‘é‡ç´¢å¼•æ˜ å°„ï¼š36ç»´åŠ¨ä½œå‘é‡ -> 128ç»´ç»Ÿä¸€å‘é‡
# é¡ºåºä¸info.jsonä¸­çš„actionä¸€è‡´ï¼šå³è‡‚6 + å³æ‰‹12 + å·¦è‡‚6 + å·¦æ‰‹12
BAAI_STATE_INDICES = [
    STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"] for i in range(6)
] + [
    STATE_VEC_IDX_MAPPING[f"right_hand_joint_{i}_pos"] for i in range(12)
] + [
    STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"] for i in range(6)
] + [
    STATE_VEC_IDX_MAPPING[f"left_hand_joint_{i}_pos"] for i in range(12)
]

# å…³èŠ‚åç§°ï¼ˆç”¨äºç»˜å›¾å’Œç»Ÿè®¡ï¼‰
JOINT_NAMES = [
    # å³è‡‚ (0-5)
    "R_Arm_J0", "R_Arm_J1", "R_Arm_J2", "R_Arm_J3", "R_Arm_J4", "R_Arm_J5",
    # å³æ‰‹ (6-17)
    "R_Hand_J0", "R_Hand_J1", "R_Hand_J2", "R_Hand_J3", "R_Hand_J4", "R_Hand_J5",
    "R_Hand_J6", "R_Hand_J7", "R_Hand_J8", "R_Hand_J9", "R_Hand_J10", "R_Hand_J11",
    # å·¦è‡‚ (18-23)
    "L_Arm_J0", "L_Arm_J1", "L_Arm_J2", "L_Arm_J3", "L_Arm_J4", "L_Arm_J5",
    # å·¦æ‰‹ (24-35)
    "L_Hand_J0", "L_Hand_J1", "L_Hand_J2", "L_Hand_J3", "L_Hand_J4", "L_Hand_J5",
    "L_Hand_J6", "L_Hand_J7", "L_Hand_J8", "L_Hand_J9", "L_Hand_J10", "L_Hand_J11",
]

# å…³èŠ‚åˆ†ç»„
JOINT_GROUPS = {
    "right_arm": list(range(0, 6)),
    "right_hand": list(range(6, 18)),
    "left_arm": list(range(18, 24)),
    "left_hand": list(range(24, 36)),
}

JOINT_GROUP_NAMES_ZH = {
    "right_arm": "å³è‡‚ (Right Arm)",
    "right_hand": "å³æ‰‹ (Right Hand)",
    "left_arm": "å·¦è‡‚ (Left Arm)",
    "left_hand": "å·¦æ‰‹ (Left Hand)",
}


# ============================================================================
# æ¨¡å‹åŠ è½½
# ============================================================================

class BAAIEvalModel:
    """ç”¨äºå¼€ç¯è¯„ä¼°çš„RDTæ¨¡å‹å°è£…ç±»"""
    
    def __init__(
        self,
        checkpoint_path: str,
        config_path: str = "configs/base.yaml",
        vision_encoder_path: str = "google/siglip-so400m-patch14-384",
        device: str = "cuda",
        dtype: torch.dtype = torch.bfloat16,
        control_frequency: int = 20,
    ):
        self.device = device
        self.dtype = dtype
        self.control_frequency = control_frequency
        
        # åŠ è½½é…ç½®
        config_file = project_root / config_path
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        print(f"ğŸ“‚ Checkpoint: {checkpoint_path}")
        print(f"ğŸ“‚ Config: {config_file}")
        
        # åŠ è½½è§†è§‰ç¼–ç å™¨
        print("ğŸ”„ åŠ è½½è§†è§‰ç¼–ç å™¨...")
        self.vision_encoder = SiglipVisionTower(
            vision_tower=vision_encoder_path, 
            args=None
        )
        self.image_processor = self.vision_encoder.image_processor
        self.vision_encoder = self.vision_encoder.to(device, dtype=dtype)
        self.vision_encoder.eval()
        print(f"   âœ… SigLIPå·²åŠ è½½, num_patches={self.vision_encoder.num_patches}")
        
        # è®¡ç®—å›¾åƒæ¡ä»¶é•¿åº¦
        img_cond_len = (
            self.config["common"]["img_history_size"] 
            * self.config["common"]["num_cameras"] 
            * self.vision_encoder.num_patches
        )
        
        # åˆ›å»ºRDTæ¨¡å‹
        print("ğŸ”„ åˆ›å»ºRDTæ¨¡å‹...")
        self.policy = RDTRunner(
            action_dim=self.config["common"]["state_dim"],
            pred_horizon=self.config["common"]["action_chunk_size"],
            config=self.config["model"],
            lang_token_dim=self.config["model"]["lang_token_dim"],
            img_token_dim=self.config["model"]["img_token_dim"],
            state_token_dim=self.config["model"]["state_token_dim"],
            max_lang_cond_len=self.config["dataset"]["tokenizer_max_length"],
            img_cond_len=img_cond_len,
            img_pos_embed_config=[
                ("image", (
                    self.config["common"]["img_history_size"],
                    self.config["common"]["num_cameras"],
                    -self.vision_encoder.num_patches
                )),
            ],
            lang_pos_embed_config=[
                ("lang", -self.config["dataset"]["tokenizer_max_length"]),
            ],
            dtype=dtype,
        )
        
        # åŠ è½½checkpointæƒé‡
        print("ğŸ”„ åŠ è½½checkpointæƒé‡...")
        self._load_checkpoint(checkpoint_path)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.policy = self.policy.to(device, dtype=dtype)
        self.policy.eval()
        
        print("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")
    
    def _load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½checkpointæƒé‡"""
        checkpoint_file = Path(checkpoint_path) / "pytorch_model.bin"
        
        if not checkpoint_file.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°checkpointæ–‡ä»¶: {checkpoint_file}")
        
        print(f"   ğŸ“¦ åŠ è½½æƒé‡: {checkpoint_file}")
        state_dict = torch.load(checkpoint_file, map_location='cpu')
        
        # å¤„ç†DeepSpeedä¿å­˜çš„state_dictæ ¼å¼
        if "module" in state_dict:
            state_dict = state_dict["module"]
        
        # ç§»é™¤å¯èƒ½çš„ "module." å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("module."):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
        
        self.policy.load_state_dict(new_state_dict, strict=False)
        print(f"   âœ… æƒé‡åŠ è½½æˆåŠŸ!")
    
    def _format_state_to_unified(self, state_36: np.ndarray) -> np.ndarray:
        """å°†36ç»´çŠ¶æ€å‘é‡æ˜ å°„åˆ°128ç»´ç»Ÿä¸€å‘é‡"""
        if state_36.ndim == 1:
            state_36 = state_36[np.newaxis, :]
        
        B, D = state_36.shape
        state_128 = np.zeros((B, self.config["common"]["state_dim"]), dtype=np.float32)
        state_128[:, BAAI_STATE_INDICES] = state_36.astype(np.float32)
        return state_128
    
    def _unformat_unified_to_state(self, action_128: np.ndarray) -> np.ndarray:
        """å°†128ç»´ç»Ÿä¸€å‘é‡æ˜ å°„å›36ç»´åŠ¨ä½œå‘é‡"""
        if action_128.ndim == 2:
            return action_128[:, BAAI_STATE_INDICES]
        elif action_128.ndim == 3:
            return action_128[:, :, BAAI_STATE_INDICES]
        return action_128[BAAI_STATE_INDICES]
    
    def preprocess_images(self, images: list) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒåˆ—è¡¨"""
        background_color = np.array([
            int(x * 255) for x in self.image_processor.image_mean
        ], dtype=np.uint8).reshape(1, 1, 3)
        background_image = np.ones((
            self.image_processor.size["height"],
            self.image_processor.size["width"], 3
        ), dtype=np.uint8) * background_color
        
        image_tensors = []
        for img in images:
            if img is None:
                img = Image.fromarray(background_image)
            elif isinstance(img, np.ndarray):
                img = Image.fromarray(img)
            
            # Pad to square
            width, height = img.size
            if width != height:
                size = max(width, height)
                new_img = Image.new(
                    img.mode, (size, size),
                    tuple(int(x * 255) for x in self.image_processor.image_mean)
                )
                new_img.paste(img, ((size - width) // 2, (size - height) // 2))
                img = new_img
            
            processed = self.image_processor.preprocess(img, return_tensors='pt')
            image_tensors.append(processed['pixel_values'][0])
        
        image_tensor = torch.stack(image_tensors, dim=0).to(self.device, dtype=self.dtype)
        
        with torch.no_grad():
            image_embeds = self.vision_encoder(image_tensor)
            image_embeds = image_embeds.reshape(-1, self.vision_encoder.hidden_size)
            image_embeds = image_embeds.unsqueeze(0)
        
        return image_embeds
    
    @torch.no_grad()
    def predict(
        self,
        state_36: np.ndarray,
        images: list,
        lang_embeds: torch.Tensor,
    ) -> np.ndarray:
        """æ‰§è¡Œæ¨ç†ï¼Œé¢„æµ‹action chunk"""
        # å‡†å¤‡çŠ¶æ€ - å…ˆè½¬ä¸ºfloat32å†è½¬ä¸ºç›®æ ‡dtypeï¼Œé¿å…numpyä¸æ”¯æŒbfloat16çš„é—®é¢˜
        state_128 = self._format_state_to_unified(state_36).astype(np.float32)
        state_tensor = torch.from_numpy(state_128).to(device=self.device, dtype=self.dtype)
        state_tensor = state_tensor.unsqueeze(1)
        
        # å‡†å¤‡çŠ¶æ€mask
        state_mask = np.zeros(self.config["common"]["state_dim"], dtype=np.float32)
        state_mask[BAAI_STATE_INDICES] = 1
        state_mask_tensor = torch.from_numpy(state_mask).to(device=self.device, dtype=self.dtype)
        state_mask_tensor = state_mask_tensor.unsqueeze(0).unsqueeze(0)
        
        # ç¼–ç å›¾åƒ
        image_embeds = self.preprocess_images(images)
        
        # å‡†å¤‡è¯­è¨€æ¡ä»¶
        if lang_embeds.ndim == 2:
            lang_embeds = lang_embeds.unsqueeze(0)
        lang_embeds = lang_embeds.to(device=self.device, dtype=self.dtype)
        lang_attn_mask = torch.ones(
            lang_embeds.shape[:2], dtype=torch.bool, device=self.device
        )
        
        # å‡†å¤‡æ§åˆ¶é¢‘ç‡
        ctrl_freqs = torch.tensor([self.control_frequency], device=self.device)
        
        # æ‰§è¡Œæ¨ç†
        predicted_actions = self.policy.predict_action(
            lang_tokens=lang_embeds,
            lang_attn_mask=lang_attn_mask,
            img_tokens=image_embeds,
            state_tokens=state_tensor,
            action_mask=state_mask_tensor,
            ctrl_freqs=ctrl_freqs,
        )
        
        # è½¬å›float32å†è½¬numpyï¼Œå› ä¸ºnumpyä¸æ”¯æŒbfloat16
        predicted_actions = predicted_actions.squeeze(0).float().cpu().numpy()
        predicted_actions_36 = self._unformat_unified_to_state(predicted_actions)
        
        return predicted_actions_36


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================

def load_episode_cache(cache_dir: str, episode_idx: int) -> dict:
    """åŠ è½½ç¼“å­˜çš„episodeæ•°æ®"""
    cache_file = Path(cache_dir) / f"episode_{episode_idx:06d}.pt"
    
    if not cache_file.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°episodeç¼“å­˜: {cache_file}")
    
    # å…¼å®¹ä¸åŒnumpyç‰ˆæœ¬
    sys.modules['numpy._core'] = np.core
    sys.modules['numpy._core.multiarray'] = np.core.multiarray
    sys.modules['numpy._core.numeric'] = getattr(np.core, 'numeric', np.core)
    
    return torch.load(cache_file, map_location='cpu', weights_only=False)


def get_sample_from_episode(
    episode_cache: dict,
    episode_idx: int,
    step_idx: int,
    chunk_size: int = 64,
) -> dict:
    """ä»episodeç¼“å­˜è·å–å•ä¸ªæ ·æœ¬"""
    qpos = episode_cache["state"]
    actions = episode_cache["action"]
    num_steps = episode_cache["frame_num"]
    images_info = episode_cache.get("images_info", {})
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œï¼Œç¡®ä¿è½¬ä¸ºfloat32
    state = np.asarray(qpos[step_idx], dtype=np.float32)
    action_gt = np.asarray(actions[step_idx:step_idx + chunk_size], dtype=np.float32)
    
    # å¦‚æœaction_gté•¿åº¦ä¸è¶³ï¼Œç”¨æœ€åä¸€ä¸ªå¡«å……
    if len(action_gt) < chunk_size:
        pad_len = chunk_size - len(action_gt)
        action_gt = np.concatenate([action_gt, np.tile(action_gt[-1:], (pad_len, 1))], axis=0)
    
    # åŠ è½½å›¾åƒ
    def load_image(cam_key, frame_idx):
        if cam_key not in images_info:
            return np.zeros((480, 640, 3), dtype=np.uint8)
        
        cam_data = images_info[cam_key]
        if isinstance(cam_data, np.ndarray):
            if frame_idx < len(cam_data):
                img = cam_data[frame_idx]
                return img.astype(np.uint8) if img.dtype != np.uint8 else img
        return np.zeros((480, 640, 3), dtype=np.uint8)
    
    # åŠ è½½2ä¸ªæ—¶é—´æ­¥ x 3ä¸ªç›¸æœºçš„å›¾åƒ
    images = []
    for t_offset in [-1, 0]:
        t = max(0, step_idx + t_offset)
        images.append(load_image('camera_head', t))
        images.append(load_image('camera_right_wrist', t))
        images.append(load_image('camera_left_wrist', t))
    
    return {
        "state": state,
        "action_gt": action_gt,
        "images": images,
        "meta": {
            "episode_idx": episode_idx,
            "step_idx": step_idx,
            "num_steps": num_steps,
        }
    }


# ============================================================================
# è¯„ä¼°æŒ‡æ ‡
# ============================================================================

def compute_metrics(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
) -> Dict[str, float]:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    mse = np.mean((action_gt - action_pred) ** 2)
    mae = np.mean(np.abs(action_gt - action_pred))
    rmse = np.sqrt(mse)
    
    # æ¯ä¸ªå…³èŠ‚çš„æŒ‡æ ‡
    mse_per_joint = np.mean((action_gt - action_pred) ** 2, axis=0)
    mae_per_joint = np.mean(np.abs(action_gt - action_pred), axis=0)
    
    # åˆ†éƒ¨ä½æŒ‡æ ‡
    metrics = {
        "mse": float(mse),
        "mae": float(mae),
        "rmse": float(rmse),
        "mse_per_joint": mse_per_joint.tolist(),
        "mae_per_joint": mae_per_joint.tolist(),
    }
    
    for group_name, indices in JOINT_GROUPS.items():
        metrics[f"mse_{group_name}"] = float(np.mean(mse_per_joint[indices]))
        metrics[f"mae_{group_name}"] = float(np.mean(mae_per_joint[indices]))
    
    return metrics


def aggregate_metrics(all_metrics: List[Dict]) -> Dict:
    """æ±‡æ€»æ‰€æœ‰æ ·æœ¬çš„æŒ‡æ ‡"""
    agg = {
        "num_samples": len(all_metrics),
        "mse_mean": np.mean([m["mse"] for m in all_metrics]),
        "mse_std": np.std([m["mse"] for m in all_metrics]),
        "mae_mean": np.mean([m["mae"] for m in all_metrics]),
        "mae_std": np.std([m["mae"] for m in all_metrics]),
        "rmse_mean": np.mean([m["rmse"] for m in all_metrics]),
        "rmse_std": np.std([m["rmse"] for m in all_metrics]),
    }
    
    # æ±‡æ€»åˆ†éƒ¨ä½æŒ‡æ ‡
    for group_name in JOINT_GROUPS.keys():
        agg[f"mse_{group_name}_mean"] = np.mean([m[f"mse_{group_name}"] for m in all_metrics])
        agg[f"mse_{group_name}_std"] = np.std([m[f"mse_{group_name}"] for m in all_metrics])
        agg[f"mae_{group_name}_mean"] = np.mean([m[f"mae_{group_name}"] for m in all_metrics])
        agg[f"mae_{group_name}_std"] = np.std([m[f"mae_{group_name}"] for m in all_metrics])
    
    # æ±‡æ€»æ¯ä¸ªå…³èŠ‚çš„æŒ‡æ ‡
    mse_per_joint_all = np.array([m["mse_per_joint"] for m in all_metrics])
    mae_per_joint_all = np.array([m["mae_per_joint"] for m in all_metrics])
    
    agg["mse_per_joint_mean"] = np.mean(mse_per_joint_all, axis=0).tolist()
    agg["mse_per_joint_std"] = np.std(mse_per_joint_all, axis=0).tolist()
    agg["mae_per_joint_mean"] = np.mean(mae_per_joint_all, axis=0).tolist()
    agg["mae_per_joint_std"] = np.std(mae_per_joint_all, axis=0).tolist()
    
    return agg


def classify_phase(step_idx: int, num_steps: int) -> str:
    """
    æ ¹æ®stepåœ¨episodeä¸­çš„ä½ç½®åˆ†ç±»é˜¶æ®µ
    
    Args:
        step_idx: å½“å‰æ­¥æ•°ç´¢å¼•
        num_steps: episodeæ€»æ­¥æ•°
    
    Returns:
        phase: "early" (å‰1/3), "mid" (ä¸­é—´1/3), "late" (å1/3)
    """
    ratio = step_idx / num_steps
    if ratio < 0.33:
        return "early"
    elif ratio < 0.67:
        return "mid"
    else:
        return "late"


def aggregate_phase_metrics(phase_metrics: Dict[str, List[Dict]]) -> Dict:
    """
    æŒ‰é˜¶æ®µæ±‡æ€»æŒ‡æ ‡
    
    Args:
        phase_metrics: {"early": [...], "mid": [...], "late": [...]}
    
    Returns:
        Dict with phase-wise aggregated metrics
    """
    phase_agg = {}
    
    for phase_name in ["early", "mid", "late"]:
        metrics_list = phase_metrics.get(phase_name, [])
        if len(metrics_list) == 0:
            continue
            
        phase_agg[phase_name] = {
            "num_samples": len(metrics_list),
            "mse_mean": float(np.mean([m["mse"] for m in metrics_list])),
            "mse_std": float(np.std([m["mse"] for m in metrics_list])),
            "mae_mean": float(np.mean([m["mae"] for m in metrics_list])),
            "mae_std": float(np.std([m["mae"] for m in metrics_list])),
        }
        
        # åˆ†éƒ¨ä½æŒ‡æ ‡
        for group_name in JOINT_GROUPS.keys():
            phase_agg[phase_name][f"mse_{group_name}"] = float(
                np.mean([m[f"mse_{group_name}"] for m in metrics_list])
            )
            phase_agg[phase_name][f"mae_{group_name}"] = float(
                np.mean([m[f"mae_{group_name}"] for m in metrics_list])
            )
    
    return phase_agg


def plot_phase_comparison(
    phase_agg: Dict,
    save_path: str,
):
    """
    ç»˜åˆ¶ä¸åŒé˜¶æ®µçš„è¯¯å·®å¯¹æ¯”å›¾
    """
    phases = ["early", "mid", "late"]
    phase_labels = ["Early (0-33%)", "Mid (33-67%)", "Late (67-100%)"]
    
    # æ£€æŸ¥å“ªäº›é˜¶æ®µæœ‰æ•°æ®
    available_phases = [p for p in phases if p in phase_agg]
    if len(available_phases) < 2:
        print("   âš ï¸  é˜¶æ®µæ•°æ®ä¸è¶³ï¼Œè·³è¿‡é˜¶æ®µå¯¹æ¯”å›¾")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10), dpi=100)
    
    # 1. æ€»ä½“MSEå¯¹æ¯”
    ax = axes[0, 0]
    mse_values = [phase_agg[p]["mse_mean"] for p in available_phases]
    mse_stds = [phase_agg[p]["mse_std"] for p in available_phases]
    labels = [phase_labels[phases.index(p)] for p in available_phases]
    
    bars = ax.bar(labels, mse_values, yerr=mse_stds, capsize=5,
                  color=['#3498db', '#2ecc71', '#e74c3c'][:len(available_phases)], alpha=0.8)
    ax.set_ylabel('MSE', fontsize=12)
    ax.set_title('MSE by Episode Phase', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    for bar, val in zip(bars, mse_values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{val:.4f}', ha='center', va='bottom', fontsize=10)
    
    # 2. æ€»ä½“MAEå¯¹æ¯”
    ax = axes[0, 1]
    mae_values = [phase_agg[p]["mae_mean"] for p in available_phases]
    mae_stds = [phase_agg[p]["mae_std"] for p in available_phases]
    
    bars = ax.bar(labels, mae_values, yerr=mae_stds, capsize=5,
                  color=['#3498db', '#2ecc71', '#e74c3c'][:len(available_phases)], alpha=0.8)
    ax.set_ylabel('MAE', fontsize=12)
    ax.set_title('MAE by Episode Phase', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    for bar, val in zip(bars, mae_values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{val:.4f}', ha='center', va='bottom', fontsize=10)
    
    # 3. åˆ†éƒ¨ä½MSEå¯¹æ¯”ï¼ˆåˆ†ç»„æŸ±çŠ¶å›¾ï¼‰
    ax = axes[1, 0]
    group_names = list(JOINT_GROUPS.keys())
    x = np.arange(len(group_names))
    width = 0.25
    
    colors = ['#3498db', '#2ecc71', '#e74c3c']
    for i, phase in enumerate(available_phases):
        values = [phase_agg[phase][f"mse_{g}"] for g in group_names]
        offset = (i - len(available_phases)/2 + 0.5) * width
        ax.bar(x + offset, values, width, label=phase_labels[phases.index(phase)],
               color=colors[i], alpha=0.8)
    
    ax.set_ylabel('MSE', fontsize=12)
    ax.set_title('MSE by Joint Group and Phase', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([JOINT_GROUP_NAMES_ZH[g] for g in group_names], fontsize=9)
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3, axis='y')
    
    # 4. æ ·æœ¬æ•°é‡ç»Ÿè®¡
    ax = axes[1, 1]
    sample_counts = [phase_agg[p]["num_samples"] for p in available_phases]
    bars = ax.bar(labels, sample_counts,
                  color=['#3498db', '#2ecc71', '#e74c3c'][:len(available_phases)], alpha=0.8)
    ax.set_ylabel('Sample Count', fontsize=12)
    ax.set_title('Sample Distribution by Phase', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    for bar, val in zip(bars, sample_counts):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{val}', ha='center', va='bottom', fontsize=12)
    
    plt.suptitle('Error Analysis by Episode Phase', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ… phase_comparison.png")


def plot_step_vs_error(
    all_metrics: List[Dict],
    save_path: str,
):
    """
    ç»˜åˆ¶step_idx vs MSEçš„æ•£ç‚¹å›¾ï¼Œåˆ†æè¯¯å·®ä¸æ—¶é—´æ­¥çš„å…³ç³»
    """
    step_indices = [m["step_idx"] for m in all_metrics]
    mse_values = [m["mse"] for m in all_metrics]
    
    # åˆ†éƒ¨ä½
    mse_right_arm = [m["mse_right_arm"] for m in all_metrics]
    mse_right_hand = [m["mse_right_hand"] for m in all_metrics]
    mse_left_arm = [m["mse_left_arm"] for m in all_metrics]
    mse_left_hand = [m["mse_left_hand"] for m in all_metrics]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10), dpi=100)
    
    # æ€»ä½“MSE
    ax = axes[0, 0]
    ax.scatter(step_indices, mse_values, alpha=0.6, c='#3498db', s=30)
    # æ·»åŠ è¶‹åŠ¿çº¿
    z = np.polyfit(step_indices, mse_values, 1)
    p = np.poly1d(z)
    x_line = np.linspace(min(step_indices), max(step_indices), 100)
    ax.plot(x_line, p(x_line), 'r--', linewidth=2, label=f'Trend (slope={z[0]:.2e})')
    ax.set_xlabel('Step Index', fontsize=12)
    ax.set_ylabel('MSE', fontsize=12)
    ax.set_title('Total MSE vs Step Index', fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # åˆ†éƒ¨ä½æ•£ç‚¹å›¾
    parts = [
        ("Right Arm", mse_right_arm, '#3498db'),
        ("Right Hand", mse_right_hand, '#2ecc71'),
        ("Left Arm", mse_left_arm, '#e74c3c'),
    ]
    
    for idx, (name, values, color) in enumerate(parts):
        ax = axes[(idx+1)//2, (idx+1)%2]
        ax.scatter(step_indices, values, alpha=0.6, c=color, s=30)
        z = np.polyfit(step_indices, values, 1)
        p = np.poly1d(z)
        ax.plot(x_line, p(x_line), 'k--', linewidth=2, label=f'Trend (slope={z[0]:.2e})')
        ax.set_xlabel('Step Index', fontsize=12)
        ax.set_ylabel('MSE', fontsize=12)
        ax.set_title(f'{name} MSE vs Step Index', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.suptitle('Error vs Time Step Analysis', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"   âœ… step_vs_error.png")


# ============================================================================
# å¯è§†åŒ–
# ============================================================================

# è¯¦ç»†å…³èŠ‚åç§°ï¼ˆç”¨äºå­å›¾æ ‡é¢˜ï¼‰
JOINT_NAMES_DETAILED = {
    "right_arm": ["R_Arm_J0 (è‚©1)", "R_Arm_J1 (è‚©2)", "R_Arm_J2 (è‚©3)", 
                  "R_Arm_J3 (è‚˜)", "R_Arm_J4 (è…•1)", "R_Arm_J5 (è…•2)"],
    "right_hand": ["R_Hand_J0", "R_Hand_J1", "R_Hand_J2", "R_Hand_J3",
                   "R_Hand_J4", "R_Hand_J5", "R_Hand_J6", "R_Hand_J7",
                   "R_Hand_J8", "R_Hand_J9", "R_Hand_J10", "R_Hand_J11"],
    "left_arm": ["L_Arm_J0 (è‚©1)", "L_Arm_J1 (è‚©2)", "L_Arm_J2 (è‚©3)",
                 "L_Arm_J3 (è‚˜)", "L_Arm_J4 (è…•1)", "L_Arm_J5 (è…•2)"],
    "left_hand": ["L_Hand_J0", "L_Hand_J1", "L_Hand_J2", "L_Hand_J3",
                  "L_Hand_J4", "L_Hand_J5", "L_Hand_J6", "L_Hand_J7",
                  "L_Hand_J8", "L_Hand_J9", "L_Hand_J10", "L_Hand_J11"],
}


def plot_detailed_joint_subplots(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
    save_dir: str,
    episode_idx: int,
    step_idx: int,
):
    """
    ä¸ºæ¯ä¸ªéƒ¨ä½ç”Ÿæˆè¯¦ç»†çš„å…³èŠ‚å­å›¾
    
    ç”Ÿæˆ4å¼ å›¾ç‰‡ï¼šright_arm, right_hand, left_arm, left_hand
    æ¯å¼ å›¾ç‰‡é‡Œæ¯ä¸ªå…³èŠ‚éƒ½æœ‰ç‹¬ç«‹çš„å­å›¾
    """
    chunk_size = action_gt.shape[0]
    timesteps = np.arange(chunk_size)
    
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    for group_name, joint_indices in JOINT_GROUPS.items():
        num_joints = len(joint_indices)
        
        # ç¡®å®šå­å›¾å¸ƒå±€
        if num_joints == 6:  # arm
            nrows, ncols = 2, 3
            figsize = (15, 8)
        else:  # hand (12 joints)
            nrows, ncols = 3, 4
            figsize = (18, 12)
        
        fig, axes = plt.subplots(nrows, ncols, figsize=figsize, dpi=100)
        axes = axes.flatten()
        
        joint_names = JOINT_NAMES_DETAILED[group_name]
        
        for i, (joint_idx, joint_name) in enumerate(zip(joint_indices, joint_names)):
            ax = axes[i]
            
            # ç»˜åˆ¶GTå’Œé¢„æµ‹
            ax.plot(timesteps, action_gt[:, joint_idx], 'b-', linewidth=2, 
                   label='GT', alpha=0.8)
            ax.plot(timesteps, action_pred[:, joint_idx], 'r--', linewidth=2,
                   label='Pred', alpha=0.8)
            
            # å¡«å……è¯¯å·®åŒºåŸŸ
            ax.fill_between(timesteps, action_gt[:, joint_idx], action_pred[:, joint_idx],
                           alpha=0.2, color='gray')
            
            # è®¡ç®—è¯¥å…³èŠ‚çš„è¯¯å·®
            mse = np.mean((action_gt[:, joint_idx] - action_pred[:, joint_idx]) ** 2)
            mae = np.mean(np.abs(action_gt[:, joint_idx] - action_pred[:, joint_idx]))
            
            ax.set_title(f"{joint_name}\nMSE: {mse:.4f}, MAE: {mae:.4f}", fontsize=10)
            ax.set_xlabel('Time Step', fontsize=9)
            ax.set_ylabel('Angle (rad)', fontsize=9)
            ax.grid(True, alpha=0.3)
            ax.legend(loc='upper right', fontsize=8)
            ax.tick_params(labelsize=8)
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(num_joints, len(axes)):
            axes[i].set_visible(False)
        
        # è®¾ç½®æ€»æ ‡é¢˜
        title = f"{JOINT_GROUP_NAMES_ZH[group_name]} - Episode {episode_idx}, Step {step_idx}"
        fig.suptitle(title, fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        save_path = save_dir / f"ep{episode_idx:04d}_step{step_idx:04d}_{group_name}.png"
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()


def save_joint_angles_json(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
    save_dir: str,
    episode_idx: int,
    step_idx: int,
):
    """
    ä¿å­˜å…³èŠ‚è§’åº¦åˆ°JSONæ–‡ä»¶
    
    åŒ…å«GTå’Œé¢„æµ‹çš„å®Œæ•´æ•°æ®ï¼ŒæŒ‰éƒ¨ä½åˆ†ç»„
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    chunk_size = action_gt.shape[0]
    
    # æ„å»ºæ•°æ®ç»“æ„
    data = {
        "meta": {
            "episode_idx": int(episode_idx),
            "step_idx": int(step_idx),
            "chunk_size": int(chunk_size),
            "num_joints": 36,
        },
        "timesteps": list(range(chunk_size)),
        "joints": {}
    }
    
    # æŒ‰éƒ¨ä½å’Œå…³èŠ‚ä¿å­˜æ•°æ®
    for group_name, joint_indices in JOINT_GROUPS.items():
        data["joints"][group_name] = {}
        joint_names = JOINT_NAMES_DETAILED[group_name]
        
        for i, (joint_idx, joint_name) in enumerate(zip(joint_indices, joint_names)):
            gt_values = action_gt[:, joint_idx].tolist()
            pred_values = action_pred[:, joint_idx].tolist()
            error = (action_gt[:, joint_idx] - action_pred[:, joint_idx]).tolist()
            
            data["joints"][group_name][f"joint_{i}"] = {
                "name": joint_name,
                "global_index": int(joint_idx),
                "gt": gt_values,
                "pred": pred_values,
                "error": error,
                "mse": float(np.mean((action_gt[:, joint_idx] - action_pred[:, joint_idx]) ** 2)),
                "mae": float(np.mean(np.abs(action_gt[:, joint_idx] - action_pred[:, joint_idx]))),
            }
    
    # æ·»åŠ æ±‡æ€»ç»Ÿè®¡
    data["summary"] = {
        "total_mse": float(np.mean((action_gt - action_pred) ** 2)),
        "total_mae": float(np.mean(np.abs(action_gt - action_pred))),
    }
    for group_name, joint_indices in JOINT_GROUPS.items():
        group_gt = action_gt[:, joint_indices]
        group_pred = action_pred[:, joint_indices]
        data["summary"][f"{group_name}_mse"] = float(np.mean((group_gt - group_pred) ** 2))
        data["summary"][f"{group_name}_mae"] = float(np.mean(np.abs(group_gt - group_pred)))
    
    # ä¿å­˜JSON
    save_path = save_dir / f"ep{episode_idx:04d}_step{step_idx:04d}_joints.json"
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    return save_path


def plot_sample_comparison(
    action_gt: np.ndarray,
    action_pred: np.ndarray,
    save_path: str,
    meta: dict = None,
):
    """ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„å¯¹æ¯”å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œæ‰€æœ‰å…³èŠ‚åœ¨ä¸€å¼ å›¾ï¼‰"""
    chunk_size, num_joints = action_gt.shape
    timesteps = np.arange(chunk_size)
    
    num_groups = len(JOINT_GROUPS)
    fig, axes = plt.subplots(num_groups, 1, figsize=(14, 3.5 * num_groups), dpi=100)
    
    if num_groups == 1:
        axes = [axes]
    
    colors_gt = plt.cm.Blues(np.linspace(0.4, 0.9, 12))
    colors_pred = plt.cm.Oranges(np.linspace(0.4, 0.9, 12))
    
    for ax_idx, (group_name, joint_indices) in enumerate(JOINT_GROUPS.items()):
        ax = axes[ax_idx]
        
        for i, joint_idx in enumerate(joint_indices):
            color_idx = i % len(colors_gt)
            
            ax.plot(
                timesteps, action_gt[:, joint_idx],
                color=colors_gt[color_idx], linestyle='-', linewidth=1.5,
                alpha=0.8
            )
            ax.plot(
                timesteps, action_pred[:, joint_idx],
                color=colors_pred[color_idx], linestyle='--', linewidth=1.5,
                alpha=0.8
            )
        
        ax.set_title(JOINT_GROUP_NAMES_ZH[group_name], fontsize=12, fontweight='bold')
        ax.set_xlabel('Time Step', fontsize=10)
        ax.set_ylabel('Joint Angle (rad)', fontsize=10)
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ å›¾ä¾‹è¯´æ˜
        ax.plot([], [], 'b-', linewidth=2, label='Ground Truth')
        ax.plot([], [], 'r--', linewidth=2, label='Prediction')
        ax.legend(loc='upper right', fontsize=9)
    
    title = "Action Prediction vs Ground Truth"
    if meta:
        title += f" (Episode {meta.get('episode_idx', 'N/A')}, Step {meta.get('step_idx', 'N/A')})"
    
    plt.suptitle(title, fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def plot_aggregate_error_bar(
    agg_metrics: Dict,
    save_path: str,
):
    """ç»˜åˆ¶åˆ†éƒ¨ä½è¯¯å·®æ¡å½¢å›¾"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=100)
    
    groups = list(JOINT_GROUPS.keys())
    group_labels = [JOINT_GROUP_NAMES_ZH[g] for g in groups]
    
    # MSE
    ax = axes[0]
    mse_means = [agg_metrics[f"mse_{g}_mean"] for g in groups]
    mse_stds = [agg_metrics[f"mse_{g}_std"] for g in groups]
    
    bars = ax.bar(group_labels, mse_means, yerr=mse_stds, capsize=5, 
                  color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], alpha=0.8)
    ax.set_ylabel('MSE', fontsize=12)
    ax.set_title('Mean Squared Error by Joint Group', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
    for bar, mean in zip(bars, mse_means):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), 
                f'{mean:.4f}', ha='center', va='bottom', fontsize=10)
    
    # MAE
    ax = axes[1]
    mae_means = [agg_metrics[f"mae_{g}_mean"] for g in groups]
    mae_stds = [agg_metrics[f"mae_{g}_std"] for g in groups]
    
    bars = ax.bar(group_labels, mae_means, yerr=mae_stds, capsize=5,
                  color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], alpha=0.8)
    ax.set_ylabel('MAE', fontsize=12)
    ax.set_title('Mean Absolute Error by Joint Group', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    for bar, mean in zip(bars, mae_means):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{mean:.4f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def plot_per_joint_error(
    agg_metrics: Dict,
    save_path: str,
):
    """ç»˜åˆ¶æ¯ä¸ªå…³èŠ‚çš„è¯¯å·®å›¾"""
    fig, axes = plt.subplots(2, 1, figsize=(16, 10), dpi=100)
    
    x = np.arange(36)
    width = 0.8
    
    # é¢œè‰²åˆ†ç»„
    colors = []
    for group_name, indices in JOINT_GROUPS.items():
        if 'right_arm' in group_name:
            colors.extend(['#3498db'] * len(indices))
        elif 'right_hand' in group_name:
            colors.extend(['#2ecc71'] * len(indices))
        elif 'left_arm' in group_name:
            colors.extend(['#e74c3c'] * len(indices))
        else:
            colors.extend(['#f39c12'] * len(indices))
    
    # MSE per joint
    ax = axes[0]
    mse_means = agg_metrics["mse_per_joint_mean"]
    mse_stds = agg_metrics["mse_per_joint_std"]
    ax.bar(x, mse_means, width, yerr=mse_stds, capsize=2, color=colors, alpha=0.8)
    ax.set_ylabel('MSE', fontsize=12)
    ax.set_title('MSE per Joint', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(JOINT_NAMES, rotation=45, ha='right', fontsize=8)
    ax.grid(True, alpha=0.3, axis='y')
    
    # MAE per joint
    ax = axes[1]
    mae_means = agg_metrics["mae_per_joint_mean"]
    mae_stds = agg_metrics["mae_per_joint_std"]
    ax.bar(x, mae_means, width, yerr=mae_stds, capsize=2, color=colors, alpha=0.8)
    ax.set_ylabel('MAE', fontsize=12)
    ax.set_title('MAE per Joint', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(JOINT_NAMES, rotation=45, ha='right', fontsize=8)
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ å›¾ä¾‹
    legend_elements = [
        plt.Rectangle((0, 0), 1, 1, facecolor='#3498db', alpha=0.8, label='å³è‡‚ (Right Arm)'),
        plt.Rectangle((0, 0), 1, 1, facecolor='#2ecc71', alpha=0.8, label='å³æ‰‹ (Right Hand)'),
        plt.Rectangle((0, 0), 1, 1, facecolor='#e74c3c', alpha=0.8, label='å·¦è‡‚ (Left Arm)'),
        plt.Rectangle((0, 0), 1, 1, facecolor='#f39c12', alpha=0.8, label='å·¦æ‰‹ (Left Hand)'),
    ]
    fig.legend(handles=legend_elements, loc='upper right', fontsize=10, 
               bbox_to_anchor=(0.98, 0.98))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def plot_error_distribution(
    all_errors: List[np.ndarray],
    save_path: str,
):
    """ç»˜åˆ¶è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾"""
    all_errors_flat = np.concatenate([e.flatten() for e in all_errors])
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=100)
    
    # æ•´ä½“è¯¯å·®åˆ†å¸ƒ
    ax = axes[0]
    ax.hist(all_errors_flat, bins=100, density=True, alpha=0.7, color='#3498db')
    ax.axvline(np.mean(all_errors_flat), color='red', linestyle='--', 
               linewidth=2, label=f'Mean: {np.mean(all_errors_flat):.4f}')
    ax.axvline(np.median(all_errors_flat), color='green', linestyle='--',
               linewidth=2, label=f'Median: {np.median(all_errors_flat):.4f}')
    ax.set_xlabel('Absolute Error', fontsize=12)
    ax.set_ylabel('Density', fontsize=12)
    ax.set_title('Error Distribution (All Joints)', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    # åˆ†ä½æ•°ç»Ÿè®¡
    ax = axes[1]
    percentiles = [50, 75, 90, 95, 99]
    values = [np.percentile(all_errors_flat, p) for p in percentiles]
    bars = ax.bar([f'{p}%' for p in percentiles], values, 
                  color=['#3498db', '#2ecc71', '#f1c40f', '#e74c3c', '#9b59b6'], alpha=0.8)
    ax.set_xlabel('Percentile', fontsize=12)
    ax.set_ylabel('Absolute Error', fontsize=12)
    ax.set_title('Error Percentiles', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    for bar, val in zip(bars, values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                f'{val:.4f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


def plot_episode_summary(
    episode_metrics: Dict[int, List[Dict]],
    save_path: str,
):
    """ç»˜åˆ¶æ¯ä¸ªepisodeçš„è¯¯å·®æ±‡æ€»å›¾"""
    episode_ids = sorted(episode_metrics.keys())
    mse_per_episode = [np.mean([m["mse"] for m in episode_metrics[ep]]) for ep in episode_ids]
    mae_per_episode = [np.mean([m["mae"] for m in episode_metrics[ep]]) for ep in episode_ids]
    
    fig, axes = plt.subplots(2, 1, figsize=(16, 8), dpi=100)
    
    # MSE per episode
    ax = axes[0]
    ax.bar(range(len(episode_ids)), mse_per_episode, alpha=0.7, color='#3498db')
    ax.axhline(np.mean(mse_per_episode), color='red', linestyle='--', 
               linewidth=2, label=f'Mean: {np.mean(mse_per_episode):.4f}')
    ax.set_xlabel('Episode Index', fontsize=12)
    ax.set_ylabel('MSE', fontsize=12)
    ax.set_title('MSE per Episode', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    # MAE per episode
    ax = axes[1]
    ax.bar(range(len(episode_ids)), mae_per_episode, alpha=0.7, color='#2ecc71')
    ax.axhline(np.mean(mae_per_episode), color='red', linestyle='--',
               linewidth=2, label=f'Mean: {np.mean(mae_per_episode):.4f}')
    ax.set_xlabel('Episode Index', fontsize=12)
    ax.set_ylabel('MAE', fontsize=12)
    ax.set_title('MAE per Episode', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="RDT-1B å¼€ç¯è¯„ä¼°è„šæœ¬")
    parser.add_argument(
        "--checkpoint", type=str, 
        default="./checkpoints/rdt1b-full-action176-20251202_000048/checkpoint-14000",
        help="Checkpointè·¯å¾„"
    )
    parser.add_argument(
        "--dataset", type=str,
        default="./data/baai/data/lerobot_baai",
        help="æ•°æ®é›†è·¯å¾„"
    )
    parser.add_argument(
        "--config", type=str,
        default="configs/base.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰"
    )
    parser.add_argument(
        "--vision_encoder", type=str,
        default="google/siglip-so400m-patch14-384",
        help="è§†è§‰ç¼–ç å™¨è·¯å¾„"
    )
    parser.add_argument(
        "--num_episodes", type=int, default=10,
        help="è¯„ä¼°çš„episodeæ•°é‡ï¼Œ-1è¡¨ç¤ºå…¨éƒ¨"
    )
    parser.add_argument(
        "--samples_per_episode", type=int, default=5,
        help="æ¯ä¸ªepisodeé‡‡æ ·çš„æ¬¡æ•°"
    )
    parser.add_argument(
        "--chunk_size", type=int, default=64,
        help="Action chunkå¤§å°"
    )
    parser.add_argument(
        "--output_dir", type=str, default="./eval_results",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device", type=str, default="cuda",
        help="è®¾å¤‡ (cuda/cpu)"
    )
    parser.add_argument(
        "--seed", type=int, default=42,
        help="éšæœºç§å­"
    )
    parser.add_argument(
        "--save_samples", action="store_true",
        help="æ˜¯å¦ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„å¯¹æ¯”å›¾"
    )
    parser.add_argument(
        "--episode_list", type=str, default=None,
        help="æŒ‡å®šè¦è¯„ä¼°çš„episodeåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 0,5,10,15"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    ckpt_name = Path(args.checkpoint).name
    output_dir = Path(args.output_dir) / f"eval_{ckpt_name}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if args.save_samples:
        samples_dir = output_dir / "samples"
        samples_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 70)
    print("ğŸ¯ RDT-1B å¼€ç¯è¯„ä¼° (Open-Loop Evaluation)")
    print("=" * 70)
    print(f"ğŸ“‚ Checkpoint: {args.checkpoint}")
    print(f"ğŸ“‚ Dataset: {args.dataset}")
    print(f"ğŸ“‚ Output: {output_dir}")
    print(f"ğŸ”¢ Episodes: {args.num_episodes} (-1 for all)")
    print(f"ğŸ”¢ Samples per episode: {args.samples_per_episode}")
    print(f"ğŸ”¢ Chunk size: {args.chunk_size}")
    print(f"ğŸ² Seed: {args.seed}")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if args.device == "cuda" and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        args.device = "cpu"
    
    if args.device == "cuda":
        print(f"ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
    
    # ä¿å­˜é…ç½®
    config_save = vars(args).copy()
    config_save["timestamp"] = timestamp
    config_save["output_dir"] = str(output_dir)
    with open(output_dir / "eval_config.json", 'w') as f:
        json.dump(config_save, f, indent=2)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("\n" + "=" * 70)
    print("ğŸš€ åˆå§‹åŒ–æ¨¡å‹")
    print("=" * 70)
    
    model = BAAIEvalModel(
        checkpoint_path=args.checkpoint,
        config_path=args.config,
        vision_encoder_path=args.vision_encoder,
        device=args.device,
        dtype=torch.bfloat16 if args.device == "cuda" else torch.float32,
        control_frequency=20,
    )
    
    # åŠ è½½è¯­è¨€åµŒå…¥
    dataset_path = Path(args.dataset)
    lang_embed_path = dataset_path / "instruction.pt"
    if lang_embed_path.exists():
        lang_embeds = torch.load(lang_embed_path, map_location='cpu')
        print(f"ğŸ“ è¯­è¨€åµŒå…¥: {lang_embeds.shape}")
    else:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¯­è¨€åµŒå…¥æ–‡ä»¶: {lang_embed_path}")
    
    # è·å–episodeåˆ—è¡¨
    cache_dir = dataset_path / "cache"
    
    if args.episode_list:
        episode_ids = [int(x.strip()) for x in args.episode_list.split(',')]
    else:
        # æ‰«æcacheç›®å½•è·å–æ‰€æœ‰episode
        all_episodes = sorted([
            int(f.stem.split('_')[1]) 
            for f in cache_dir.glob("episode_*.pt")
            if f.stem != "episode_metadata"
        ])
        
        if args.num_episodes == -1 or args.num_episodes >= len(all_episodes):
            episode_ids = all_episodes
        else:
            episode_ids = random.sample(all_episodes, args.num_episodes)
            episode_ids.sort()
    
    print(f"\nğŸ“‹ å°†è¯„ä¼° {len(episode_ids)} ä¸ªepisodes: {episode_ids[:10]}{'...' if len(episode_ids) > 10 else ''}")
    
    # å¼€å§‹è¯„ä¼°
    print("\n" + "=" * 70)
    print("ğŸ”„ å¼€å§‹è¯„ä¼°")
    print("=" * 70)
    
    all_metrics = []
    all_errors = []
    episode_metrics = {}
    phase_metrics = {"early": [], "mid": [], "late": []}  # æŒ‰é˜¶æ®µåˆ†ç±»çš„æŒ‡æ ‡
    
    for episode_idx in tqdm(episode_ids, desc="Evaluating episodes"):
        try:
            episode_cache = load_episode_cache(str(cache_dir), episode_idx)
        except Exception as e:
            print(f"\nâš ï¸  åŠ è½½episode {episode_idx} å¤±è´¥: {e}")
            continue
        
        num_steps = episode_cache["frame_num"]
        qpos = episode_cache["state"]
        
        # æ‰¾åˆ°è¿åŠ¨èµ·å§‹ç‚¹
        EPS = 1e-2
        qpos_delta = np.abs(qpos - qpos[0:1])
        indices = np.where(np.any(qpos_delta > EPS, axis=1))[0]
        first_idx = indices[0] if len(indices) > 0 else 1
        
        # ç¡®å®šå¯é‡‡æ ·èŒƒå›´
        max_valid_step = max(first_idx, num_steps - args.chunk_size - 1)
        
        if max_valid_step <= first_idx:
            print(f"\nâš ï¸  Episode {episode_idx} æ­¥æ•°ä¸è¶³ï¼Œè·³è¿‡")
            continue
        
        # åœ¨è¯¥episodeä¸Šé‡‡æ · - å‡åŒ€åˆ†å¸ƒåœ¨æ•´ä¸ªepisodeä¸Š
        sample_steps = np.linspace(first_idx, max_valid_step, args.samples_per_episode, dtype=int)
        sample_steps = np.unique(sample_steps)
        
        episode_metrics[episode_idx] = []
        
        for step_idx in sample_steps:
            try:
                sample = get_sample_from_episode(
                    episode_cache, episode_idx, step_idx, args.chunk_size
                )
                
                # æ‰§è¡Œæ¨ç†
                with torch.inference_mode():
                    action_pred = model.predict(
                        state_36=sample["state"],
                        images=sample["images"],
                        lang_embeds=lang_embeds,
                    )
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = compute_metrics(sample["action_gt"], action_pred)
                metrics["episode_idx"] = episode_idx
                metrics["step_idx"] = step_idx
                metrics["num_steps"] = num_steps  # è®°å½•episodeæ€»æ­¥æ•°
                
                # åˆ†ç±»é˜¶æ®µ
                phase = classify_phase(step_idx, num_steps)
                metrics["phase"] = phase
                
                all_metrics.append(metrics)
                episode_metrics[episode_idx].append(metrics)
                phase_metrics[phase].append(metrics)  # æŒ‰é˜¶æ®µæ”¶é›†
                
                # æ”¶é›†è¯¯å·®ç”¨äºåˆ†å¸ƒå›¾
                error = np.abs(sample["action_gt"] - action_pred)
                all_errors.append(error)
                
                # ä¿å­˜æ ·æœ¬å¯¹æ¯”å›¾å’ŒJSON
                if args.save_samples:
                    # 1. ä¿å­˜4å¼ è¯¦ç»†çš„åˆ†éƒ¨ä½å­å›¾ï¼ˆright_arm, right_hand, left_arm, left_handï¼‰
                    plot_detailed_joint_subplots(
                        sample["action_gt"], action_pred,
                        str(samples_dir),
                        episode_idx, step_idx
                    )
                    
                    # 2. ä¿å­˜å…³èŠ‚è§’JSONæ–‡ä»¶
                    save_joint_angles_json(
                        sample["action_gt"], action_pred,
                        str(samples_dir),
                        episode_idx, step_idx
                    )
                    
                    # 3. ä¿å­˜ç®€åŒ–ç‰ˆæ±‡æ€»å›¾ï¼ˆå¯é€‰ï¼Œä¿ç•™å…¼å®¹æ€§ï¼‰
                    plot_sample_comparison(
                        sample["action_gt"], action_pred,
                        str(samples_dir / f"ep{episode_idx:04d}_step{step_idx:04d}_overview.png"),
                        meta=sample["meta"]
                    )
                    
            except Exception as e:
                print(f"\nâš ï¸  Episode {episode_idx} Step {step_idx} æ¨ç†å¤±è´¥: {e}")
                continue
    
    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
    print("=" * 70)
    
    if len(all_metrics) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ ·æœ¬ï¼")
        return
    
    agg_metrics = aggregate_metrics(all_metrics)
    
    print(f"\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡ ({agg_metrics['num_samples']} ä¸ªæ ·æœ¬):")
    print(f"   MSE:  {agg_metrics['mse_mean']:.6f} Â± {agg_metrics['mse_std']:.6f}")
    print(f"   MAE:  {agg_metrics['mae_mean']:.6f} Â± {agg_metrics['mae_std']:.6f}")
    print(f"   RMSE: {agg_metrics['rmse_mean']:.6f} Â± {agg_metrics['rmse_std']:.6f}")
    
    print(f"\nğŸ“ˆ åˆ†éƒ¨ä½MSE:")
    for group_name in JOINT_GROUPS.keys():
        mean = agg_metrics[f"mse_{group_name}_mean"]
        std = agg_metrics[f"mse_{group_name}_std"]
        print(f"   {JOINT_GROUP_NAMES_ZH[group_name]}: {mean:.6f} Â± {std:.6f}")
    
    # æŒ‰é˜¶æ®µæ±‡æ€»
    phase_agg = aggregate_phase_metrics(phase_metrics)
    
    print(f"\nğŸ“ˆ åˆ†é˜¶æ®µMSE:")
    phase_names_zh = {"early": "åˆæœŸ (0-33%)", "mid": "ä¸­æœŸ (33-67%)", "late": "æœ«æœŸ (67-100%)"}
    for phase_name in ["early", "mid", "late"]:
        if phase_name in phase_agg:
            p = phase_agg[phase_name]
            print(f"   {phase_names_zh[phase_name]}: MSE={p['mse_mean']:.6f} Â± {p['mse_std']:.6f} ({p['num_samples']} samples)")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    print("\n" + "=" * 70)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print("=" * 70)
    
    # åˆ†éƒ¨ä½è¯¯å·®æ¡å½¢å›¾
    plot_aggregate_error_bar(agg_metrics, str(output_dir / "error_by_group.png"))
    print("   âœ… error_by_group.png")
    
    # æ¯ä¸ªå…³èŠ‚çš„è¯¯å·®å›¾
    plot_per_joint_error(agg_metrics, str(output_dir / "error_per_joint.png"))
    print("   âœ… error_per_joint.png")
    
    # è¯¯å·®åˆ†å¸ƒå›¾
    plot_error_distribution(all_errors, str(output_dir / "error_distribution.png"))
    print("   âœ… error_distribution.png")
    
    # Episodeæ±‡æ€»å›¾
    if len(episode_metrics) > 1:
        plot_episode_summary(episode_metrics, str(output_dir / "error_per_episode.png"))
        print("   âœ… error_per_episode.png")
    
    # é˜¶æ®µå¯¹æ¯”å›¾
    if len(phase_agg) >= 2:
        plot_phase_comparison(phase_agg, str(output_dir / "phase_comparison.png"))
    
    # Step vs Erroræ•£ç‚¹å›¾
    if len(all_metrics) >= 10:
        plot_step_vs_error(all_metrics, str(output_dir / "step_vs_error.png"))
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results = {
        "config": config_save,
        "aggregate_metrics": agg_metrics,
        "phase_metrics": phase_agg,  # æ·»åŠ é˜¶æ®µæŒ‡æ ‡
        "all_metrics": all_metrics,
    }
    
    # è‡ªå®šä¹‰JSONç¼–ç å™¨å¤„ç†numpyç±»å‹
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return super().default(obj)
    
    # ä¿å­˜JSONæ ¼å¼
    with open(output_dir / "results.json", 'w') as f:
        json.dump(results, f, indent=2, cls=NumpyEncoder)
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜: {output_dir / 'results.json'}")
    
    # ä¿å­˜NPZæ ¼å¼ï¼ˆåŒ…å«numpyæ•°ç»„ï¼‰
    np.savez(
        output_dir / "results.npz",
        aggregate_metrics=agg_metrics,
        mse_per_joint_mean=np.array(agg_metrics["mse_per_joint_mean"]),
        mae_per_joint_mean=np.array(agg_metrics["mae_per_joint_mean"]),
    )
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {output_dir / 'results.npz'}")
    
    print("\n" + "=" * 70)
    print("âœ… å¼€ç¯è¯„ä¼°å®Œæˆ!")
    print("=" * 70)
    print(f"\nğŸ“‚ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")


if __name__ == "__main__":
    main()

