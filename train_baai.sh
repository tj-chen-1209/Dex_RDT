#!/bin/bash
# ==============================================================================
# RDT-1B å…¨é‡å¾®è°ƒè®­ç»ƒè„šæœ¬ - é’ˆå¯¹ A800*7 ä¼˜åŒ–
# ==============================================================================
# æ•°æ®é›†ï¼šlerobot_baai (100 episodes, LeRobotæ ¼å¼)
# ç¡¬ä»¶ï¼š7x NVIDIA A800 (80GB each, æŽ’é™¤GPU:0)
# ä½œè€…ï¼šAI Assistant
# æ—¥æœŸï¼š$(date +%Y-%m-%d)
# ==============================================================================

export run_id=$(date +%Y%m%d_%H%M%S)

# ====== NCCL é€šä¿¡é…ç½®ï¼ˆå¤šå¡è®­ç»ƒä¼˜åŒ–ï¼‰======
export NCCL_IB_HCA=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_7:1,mlx5_8:1,mlx5_9:1
export NCCL_IB_DISABLE=0
export NCCL_SOCKET_IFNAME=bond0
export NCCL_DEBUG=INFO
export NCCL_NVLS_ENABLE=0
export DS_BUILD_EVOFORMER_ATTN=0

# ====== ç¼–è¯‘çŽ¯å¢ƒé…ç½® ======
export CFLAGS="-I/usr/include"
export LDFLAGS="-L/usr/lib/x86_64-linux-gnu"

# ====== æ¨¡åž‹ç¼–ç å™¨è·¯å¾„ ======
export TEXT_ENCODER_NAME="google/t5-v1_1-xxl"
export VISION_ENCODER_NAME="google/siglip-so400m-patch14-384"

# ====== è®­ç»ƒè¶…å‚æ•°é…ç½® ======
dataset_name="baai"
action_name="lerobot_baai"
dataset_source="lerobot"  # æ•°æ®æº: 'bson' æˆ– 'lerobot'
model_type="full"
lr="1e-4"
train_batch_size=48
gradient_accumulation_steps=2
sample_batch_size=32
num_sample_batches=4
seed=42
max_train_steps=200000
checkpointing_period=1000
sample_period=500

# ====== Resume Training é…ç½® ======
# ç•™ç©ºè¡¨ç¤ºä»Žå¤´å¼€å§‹è®­ç»ƒï¼Œè®¾ç½®è·¯å¾„è¡¨ç¤ºä»Žcheckpointæ¢å¤è®­ç»ƒ
# ä¾‹å¦‚: resume_checkpoint="./checkpoints/rdt1b-full-action176-20251202_000048/checkpoint-4000"
# æˆ–è€…: resume_checkpoint="latest" (è‡ªåŠ¨ä½¿ç”¨æœ€æ–°çš„checkpoint)
resume_checkpoint="./checkpoints/rdt1b-full-action176-20251202_000048/checkpoint-14000"

# ç”Ÿæˆæ¸…æ™°çš„è¾“å‡ºè·¯å¾„
# å¦‚æžœæ˜¯ resume trainingï¼Œä½¿ç”¨åŽŸæ¥çš„è¾“å‡ºç›®å½•ï¼›å¦åˆ™åˆ›å»ºæ–°ç›®å½•
if [ -n "$resume_checkpoint" ] && [ "$resume_checkpoint" != "latest" ]; then
    # ä»Ž checkpoint è·¯å¾„æå–åŽŸå§‹è¾“å‡ºç›®å½•
    # ä¾‹å¦‚: ./checkpoints/rdt1b-full-action176-20251202_000048/checkpoint-4000
    # æå–: ./checkpoints/rdt1b-full-action176-20251202_000048
    export OUTPUT_DIR=$(dirname "$resume_checkpoint")
    echo "ðŸ“‚ ä½¿ç”¨åŽŸæœ‰è¾“å‡ºç›®å½•ï¼ˆResumeæ¨¡å¼ï¼‰: $OUTPUT_DIR"
else
    export OUTPUT_DIR="./checkpoints/rdt1b-${model_type}-${action_name}-${run_id}"
    echo "ðŸ“‚ åˆ›å»ºæ–°è¾“å‡ºç›®å½•: $OUTPUT_DIR"
fi

# ====== åˆ›å»ºè¾“å‡ºç›®å½•å’Œé…ç½®æ–‡ä»¶ ======
if [ ! -d "$OUTPUT_DIR" ]; then
    mkdir -p "$OUTPUT_DIR"
    echo "âœ… Output folder '$OUTPUT_DIR' created"
    
    # åˆ›å»ºè¯¦ç»†é…ç½®è¯´æ˜Žæ–‡ä»¶
    cat > "$OUTPUT_DIR/training_config.txt" <<EOF
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          RDT-1B Full Fine-tuning Configuration                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“‹ åŸºæœ¬ä¿¡æ¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Run ID: ${run_id}
  Model: RDT-1B (1 Billion parameters)
  Method: Full Fine-tuning (å…¨é‡å¾®è°ƒ - æ‰€æœ‰å‚æ•°å¯è®­ç»ƒ)
  Training Mode: $([ -n "$resume_checkpoint" ] && echo "Resume Training (æ¢å¤è®­ç»ƒ)" || echo "New Training (æ–°è®­ç»ƒ)")
  Resume From: $([ -n "$resume_checkpoint" ] && echo "$resume_checkpoint" || echo "N/A")
  Dataset: ${dataset_name}/${action_name} (100 episodes)
  Dataset Source: ${dataset_source} (LeRobotæ ¼å¼)
  Hardware: 7x NVIDIA A800 (80GB VRAM each, GPU:0 excluded)
  Random Seed: ${seed}

ðŸŽ¯ è®­ç»ƒè¶…å‚æ•°
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Learning Rate: ${lr} (å›ºå®šå­¦ä¹ çŽ‡)
  LR Scheduler: constant (ä¸ä½¿ç”¨è°ƒåº¦å™¨)
  Per-Device Batch Size: ${train_batch_size}
  Gradient Accumulation Steps: ${gradient_accumulation_steps}
  Effective Batch Size: $((train_batch_size * gradient_accumulation_steps * 6)) (global, 6 GPUs)
  Max Training Steps: ${max_train_steps}
  Checkpointing Period: ${checkpointing_period} steps
  Sample Period: ${sample_period} steps
  Checkpoints Keep: 20 (æœ€è¿‘çš„)
  Mixed Precision: bf16 (bfloat16)
  Optimizer: 8-bit Adam (èŠ‚çœæ˜¾å­˜)
  Max Gradient Norm: 1.0

ðŸ”§ DeepSpeed é…ç½®
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ZeRO Stage: 2 (Optimizer + Gradient Partitioning)
  Overlap Communication: Yes
  Contiguous Gradients: Yes

ðŸ“Š æ•°æ®é…ç½®
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Dataset Source: ${dataset_source} (LeRobotæ ¼å¼)
  Dataset Path: data/baai/data/lerobot_baai
  Precomputed Language Embeddings: Yes (èŠ‚çœè®¡ç®—)
  Image History Size: 2 frames
  Number of Cameras: 3 (RDT-1Bæ¨¡åž‹é™åˆ¶)
  Image Augmentation: Enabled (ColorJitter, Blur, Noise)
  State Noise SNR: 40 dB
  Condition Mask Probability: 0.1
  Dataloader Workers: 8

ðŸ’¾ æ˜¾å­˜ä¼°ç®—ï¼ˆå•å¡A800 80GBï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Model Parameters (bf16): ~2.0 GB
  Optimizer States (8-bit): ~1.5 GB
  Gradients (ZeRO-2): ~0.4 GB
  Activations (batch=8): ~15-20 GB
  Working Memory: ~5 GB
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total Estimated: ~25-30 GB / 80 GB âœ… æ˜¾å­˜å……è¶³

ðŸ“ˆ è®­ç»ƒè¿›åº¦ä¼°ç®—
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  æ•°æ®é›†å¤§å°: 100 episodes Ã— ~604KB â‰ˆ 60 MB
  æ¯æ­¥æ ·æœ¬æ•°: 96 (å…¨å±€batch size)
  æ€»è®­ç»ƒæ­¥æ•°: ${max_train_steps}
  é¢„è®¡è®­ç»ƒæ—¶é—´: ~8-12å°æ—¶ (å–å†³äºŽæ•°æ®åŠ è½½é€Ÿåº¦)
  Checkpointæ€»å¤§å°: ~80 GB (20ä¸ªcheckpoints Ã— 4GB each)

ðŸš€ å¯åŠ¨å‘½ä»¤
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  bash train_baai_optimized.sh

ðŸ“ ç›‘æŽ§è®­ç»ƒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  TensorBoard: tensorboard --logdir=${OUTPUT_DIR}
  è®¿é—®åœ°å€: http://localhost:6006

Started: $(date)
EOF
else
    echo "âš ï¸  Output folder '$OUTPUT_DIR' already exists"
fi

# ====== æ˜¾ç¤ºè®­ç»ƒé…ç½®æ‘˜è¦ ======
echo ""
if [ -n "$resume_checkpoint" ]; then
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘          ðŸ”„ RDT-1B Resume Training on A800*7                     â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
else
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘          ðŸš€ RDT-1B Full Fine-tuning on A800*7                    â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
fi
echo ""
echo "ðŸ“¦ æ•°æ®é›†: ${dataset_name}/${action_name} (100 episodes)"
echo "ðŸ“‚ æ•°æ®æº: ${dataset_source} (LeRobotæ ¼å¼)"
echo "ðŸŽ¯ æ¨¡åž‹: RDT-1B (1B params, full fine-tuning)"
echo "ðŸ’» ç¡¬ä»¶: 7x A800 GPUs (GPU:0 excluded)"
echo "ðŸ“Š å…¨å±€Batch Size: $((train_batch_size * gradient_accumulation_steps * 7))"
echo "ðŸ“ˆ è®­ç»ƒæ­¥æ•°: ${max_train_steps}"
echo "ðŸ’¾ è¾“å‡ºç›®å½•: ${OUTPUT_DIR}"
if [ -n "$resume_checkpoint" ]; then
    echo "ðŸ”„ æ¢å¤ç‚¹: ${resume_checkpoint}"
fi
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# ====== å¯åŠ¨è®­ç»ƒï¼ˆDeepSpeedåˆ†å¸ƒå¼è®­ç»ƒï¼‰======
# æž„å»º resume checkpoint å‚æ•°
if [ -n "$resume_checkpoint" ]; then
    resume_arg="--resume_from_checkpoint=$resume_checkpoint"
    echo "ðŸ”„ Resume Training Mode: ä»Ž checkpoint æ¢å¤è®­ç»ƒ"
    echo "ðŸ“‚ Checkpoint Path: $resume_checkpoint"
    echo ""
else
    resume_arg=""
    echo "ðŸ†• New Training Mode: ä»Žå¤´å¼€å§‹è®­ç»ƒ"
    echo ""
fi

# deepspeed --exclude="localhost:0" main_baai.py \
deepspeed --hostfile=hostfile.txt main_baai.py \
    --deepspeed="./configs/zero2.json" \
    --pretrained_model_name_or_path="./checkpoints/rdt-1b" \
    --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
    --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
    --output_dir=$OUTPUT_DIR \
    $resume_arg \
    --seed=${seed} \
    --train_batch_size=${train_batch_size} \
    --gradient_accumulation_steps=${gradient_accumulation_steps} \
    --sample_batch_size=${sample_batch_size} \
    --num_sample_batches=${num_sample_batches} \
    --max_train_steps=${max_train_steps} \
    --checkpointing_period=${checkpointing_period} \
    --sample_period=${sample_period} \
    --checkpoints_total_limit=40 \
    --lr_scheduler="constant" \
    --learning_rate=${lr} \
    --mixed_precision="bf16" \
    --dataloader_num_workers=8 \
    --image_aug \
    --dataset_type="finetune" \
    --state_noise_snr=40 \
    --dataset_source=${dataset_source} \
    --report_to=wandb \
    --precomp_lang_embed
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "âœ… Training completed or interrupted!"
echo "ðŸ“ Results saved to: ${OUTPUT_DIR}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"